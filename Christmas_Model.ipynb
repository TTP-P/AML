{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe1e597",
   "metadata": {},
   "source": [
    "# ----Lightgbm-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7fe26a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LIGHTGBM_SUPPRESS_WARNINGS'] = '1'\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152c32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Performing GPU Pre-flight Check...\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 10, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.471304\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      " GPU Pre-flight Check PASSED. LightGBM can access the GPU.\n",
      "\n",
      "\n",
      "== RUNNING REGRESSION MODELS IN OOT VALIDATION MODE ===\n",
      "Added 'is_christmas' feature to dataset\n",
      "  Train: <= 2021-12-31 (364377 rows)\n",
      "  Val:   2021-12-31 < Date <= 2022-12-31 (72988 rows)\n",
      "  Test:  > 2022-12-31 (29696 rows)\n",
      "\n",
      "Christmas weeks distribution:\n",
      "  Train: 37998 / 364377 (10.4%)\n",
      "  Val:   7594 / 72988 (10.4%)\n",
      "  Test:  1393 / 29696 (4.7%)\n",
      "Cached 2 training subsets\n",
      "\n",
      "\n",
      "===== Processing Regression Target: Points_next_week =====\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Feature Set: baseline\n",
      "Features: 19 (baseline only)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "--- Hyperparameter Search for baseline ---\n",
      "\n",
      "Starting Hyperparameter Search (Mode: Regression)\n",
      "\n",
      "Fitting RandomizedSearchCV with early stopping...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[151]\tvalid_0's l1: 9.32485\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, num_leaves=32, random_state=42, reg_lambda=9, subsample=0.9; total time=  12.5s\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid_0's l1: 9.35302\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=8, n_estimators=300, num_leaves=64, random_state=42, reg_lambda=9, subsample=0.7; total time=  13.7s\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's l1: 9.13651\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=8, n_estimators=300, num_leaves=64, random_state=42, reg_lambda=9, subsample=0.7; total time=  16.7s\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[232]\tvalid_0's l1: 9.12782\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, num_leaves=32, random_state=42, reg_lambda=9, subsample=0.9; total time=  20.3s\n",
      "Early stopping, best iteration is:\n",
      "[230]\tvalid_0's l1: 9.32788\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, n_estimators=300, num_leaves=32, random_state=42, reg_lambda=5, subsample=1.0; total time=  22.4s\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[294]\tvalid_0's l1: 9.06127\n",
      "Early stopping, best iteration is:\n",
      "[126]\tvalid_0's l1: 9.05992\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=8, n_estimators=300, num_leaves=64, random_state=42, reg_lambda=9, subsample=0.7; total time=  24.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, num_leaves=32, random_state=42, reg_lambda=9, subsample=0.9; total time=  24.4s\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "from sklearn.inspection import permutation_importance\n",
    "import time\n",
    "\n",
    "\n",
    "# Configuration\n",
    "input_filename = 'Spotify_Model_Ready_Features_V2.csv'\n",
    "use_gpu = True \n",
    "\n",
    "# split dates\n",
    "TRAIN_END = pd.Timestamp('2021-12-31')\n",
    "VAL_END = pd.Timestamp('2022-12-31')\n",
    "\n",
    "BASELINE_FEATURES = [\n",
    "    'Danceability', 'Energy', 'Loudness_Corrected', 'Speechiness',\n",
    "    'Acousticness', 'Instrumentalness', 'Valence', 'Artist_Count',\n",
    "    'Nationality_Count', 'Rank', 'Points (Total)', 'Rank_last_week',\n",
    "    'Points_last_week', 'Rank_change', 'Points_change',\n",
    "    'Points_rolling_mean_4w', 'Rank_rolling_mean_4w',\n",
    "    'Weeks_on_chart', 'Artist_Hotness'\n",
    "]\n",
    "CHRISTMAS_FEATURES = BASELINE_FEATURES + ['is_christmas']\n",
    "\n",
    "\n",
    "def mark_christmas_period(date_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Mark December and first week of January as Christmas period.\"\"\"\n",
    "    december = date_series.dt.month == 12\n",
    "    january_first_week = (date_series.dt.month == 1) & (date_series.dt.day <= 7)\n",
    "    return (december | january_first_week).astype(int)\n",
    "\n",
    "\n",
    "def train_regression_pipeline(\n",
    "    df_train, df_val, df_oot, feature_columns, target_column, model_params,\n",
    "    model_name=\"\", save_detailed_predictions=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a LightGBM regression model with Christmas period segmentation.\n",
    "    \n",
    "    Data splits:\n",
    "    - df_train: Training data (used to fit model)\n",
    "    - df_val: Validation data (used for early stopping)\n",
    "    - df_test: Test data (final evaluation,)\n",
    "    \n",
    "    Evaluates performance on both val and test sets, with segment breakdown:\n",
    "    - All data\n",
    "    - Christmas weeks only\n",
    "    - Non-Christmas weeks only\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output_dir = os.path.join(\"results\", \"regression\", \"Christmas_Model\", target_column, model_name)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        suffix = target_column.replace('Points_', '')\n",
    "        \n",
    "        metrics_output_filename = os.path.join(output_dir, f\"metrics_{suffix}_oot.csv\")\n",
    "        importance_output_filename = os.path.join(output_dir, f\"importance_{suffix}.csv\")\n",
    "        model_output_filename = os.path.join(output_dir, f\"model_{suffix}.pkl\")\n",
    "        oot_predictions_output_filename = os.path.join(output_dir, f\"oot_predictions_and_actuals.csv\")\n",
    "        rank_metrics_filename = os.path.join(output_dir, f\"metrics_derived_rank_oot.csv\")\n",
    "        \n",
    "\n",
    "        print(f\"\\nStep 5: Training model for {target_column} ({model_name})\")\n",
    "    \n",
    "        # Prepare training data\n",
    "        df_train_target = df_train.dropna(subset=[target_column]).copy()\n",
    "        X_train = df_train_target[feature_columns]\n",
    "        y_train = df_train_target[target_column]\n",
    "        \n",
    "        if X_train.empty:\n",
    "            print(f\"Skipping {target_column}: No training data available after dropna.\")\n",
    "            return\n",
    "        \n",
    "        # Prepare validation data (for early stopping)\n",
    "        df_val_target = df_val.dropna(subset=[target_column]).copy()\n",
    "        X_val = df_val_target[feature_columns]\n",
    "        y_val = df_val_target[target_column]\n",
    "        \n",
    "        if X_val.empty:\n",
    "            print(f\"Warning: Validation set for {target_column} is empty.\")\n",
    "            return\n",
    "        \n",
    "        # Train model with validation set for early stopping\n",
    "        start_time = time.time()\n",
    "        final_model = lgb.LGBMRegressor(**model_params)\n",
    "        \n",
    "        # Reuse static callbacks to reduce overhead\n",
    "        if not hasattr(train_regression_pipeline, \"_callbacks\"):\n",
    "            train_regression_pipeline._callbacks = [\n",
    "                early_stopping(stopping_rounds=30),\n",
    "                log_evaluation(period=0)\n",
    "            ]\n",
    "        callbacks = train_regression_pipeline._callbacks\n",
    "        \n",
    "        final_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"mae\",\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        train_duration = end_time - start_time\n",
    "        runtime_log_path = os.path.join(output_dir, f\"runtime_log_{suffix}.csv\")\n",
    "        pd.DataFrame({\n",
    "            'model': [model_name],\n",
    "            'target': [target_column],\n",
    "            'train_time_sec': [train_duration]\n",
    "        }).to_csv(runtime_log_path, sep=';', index=False)\n",
    "        print(f\" Training duration logged to '{runtime_log_path}' ({train_duration:.1f} sec)\")\n",
    "        \n",
    "        train_curve = final_model.evals_result_\n",
    "        if train_curve and 'training' in train_curve and 'l1' in train_curve['training']:\n",
    "            curve_df = pd.DataFrame({\n",
    "                'iteration': range(len(train_curve['training']['l1'])),\n",
    "                'train_mae': train_curve['training']['l1'],\n",
    "                'val_mae': train_curve['valid_0']['l1']\n",
    "            })\n",
    "            curve_path = os.path.join(output_dir, f\"training_curve_{suffix}.csv\")\n",
    "            curve_df.to_csv(curve_path, sep=';', index=False)\n",
    "            print(f\"Training curve saved to '{curve_path}'\")\n",
    "        \n",
    "        try:\n",
    "            device_type = final_model.booster_.params.get(\"device_type\", \"unknown\")\n",
    "            print(f\"Training complete. Best iteration: {final_model.best_iteration_} | Device used: {device_type}\")\n",
    "        except Exception as e:\n",
    "            print(\"Unable to detect device type:\", e)\n",
    "\n",
    "        joblib.dump(final_model, model_output_filename)\n",
    "        print(f\"Final model saved to '{model_output_filename}'\")\n",
    "        \n",
    "        print(\"\\nStep 7: Performing Out-of-Time (OOT) Hold-Out Testing\")\n",
    "\n",
    "        df_oot_target = df_oot.dropna(subset=[target_column]).copy()\n",
    "        X_oot = df_oot_target[feature_columns]\n",
    "        y_oot = df_oot_target[target_column]\n",
    "\n",
    "        if X_oot.empty:\n",
    "            print(f\"Warning: OOT set for {target_column} is empty. Skipping OOT evaluation.\")\n",
    "            return\n",
    "\n",
    "        oot_predictions = final_model.predict(X_oot)\n",
    "\n",
    "        # Derive and Evaluate Ranks from Points Predictions\n",
    "        print(\"\\n--- Deriving and Evaluating Ranks from Points Predictions ---\")\n",
    "\n",
    "        # 创建 DataFrame 保留日期，用于每周分组排序\n",
    "        results_df = df_oot_target[['Date']].copy()\n",
    "        results_df['true_points'] = y_oot\n",
    "        results_df['predicted_points'] = oot_predictions\n",
    "\n",
    "        # 获取真实排名列用于对比\n",
    "        true_rank_col_name = target_column.replace('Points', 'Rank')\n",
    "        results_df['true_rank'] = df_oot_target[true_rank_col_name]\n",
    "\n",
    "        # 按每周积分降序排序生成预测排名\n",
    "        results_df['predicted_rank'] = results_df.groupby('Date')['predicted_points'].rank(\n",
    "            method='first', ascending=False\n",
    "        )\n",
    "\n",
    "        # 计算与积分一致的四个指标\n",
    "        mae_rank = mean_absolute_error(results_df['true_rank'], results_df['predicted_rank'])\n",
    "        rmse_rank = np.sqrt(mean_squared_error(results_df['true_rank'], results_df['predicted_rank']))\n",
    "        r2_rank = r2_score(results_df['true_rank'], results_df['predicted_rank'])\n",
    "        spearman_rank, _ = spearmanr(results_df['true_rank'], results_df['predicted_rank'])\n",
    "\n",
    "        print(\"\\n--- Derived Rank Evaluation (Aligned with Points Metrics) ---\")\n",
    "        print(f\"MAE (Rank): {mae_rank:.3f}\")\n",
    "        print(f\"RMSE (Rank): {rmse_rank:.3f}\")\n",
    "        print(f\"R² (Rank): {r2_rank:.3f}\")\n",
    "        print(f\"Spearman (Rank): {spearman_rank:.3f}\")\n",
    "\n",
    "        # Compute metrics\n",
    "        mae_oot = mean_absolute_error(y_oot, oot_predictions)\n",
    "        r2_oot = r2_score(y_oot, oot_predictions)\n",
    "        spearman_oot, _ = spearmanr(y_oot, oot_predictions)\n",
    "        rmse_oot = np.sqrt(mean_squared_error(y_oot, oot_predictions))\n",
    "        \n",
    "        def compute_metric_std(y_true, y_pred, n_splits=3):\n",
    "            size = len(y_true) // n_splits\n",
    "            maes, r2s, rmses, spearmans = [], [], [], []\n",
    "            for i in range(n_splits):\n",
    "                start, end = i * size, (i + 1) * size\n",
    "                y_t, y_p = y_true[start:end], y_pred[start:end]\n",
    "                if len(y_t) == 0: continue\n",
    "                maes.append(mean_absolute_error(y_t, y_p))\n",
    "                r2s.append(r2_score(y_t, y_p))\n",
    "                rmses.append(np.sqrt(mean_squared_error(y_t, y_p)))\n",
    "                # Handle potential errors in Spearman calculation on small chunks\n",
    "                if len(np.unique(y_t)) > 1 and len(np.unique(y_p)) > 1:\n",
    "                    spearmans.append(spearmanr(y_t, y_p)[0])\n",
    "                else:\n",
    "                    spearmans.append(np.nan)\n",
    "            \n",
    "            return {\n",
    "                'MAE_std': np.nanstd(maes),\n",
    "                'R2_std': np.nanstd(r2s),\n",
    "                'RMSE_std': np.nanstd(rmses),\n",
    "                'Spearman_std': np.nanstd(spearmans)\n",
    "            }\n",
    "        \n",
    "        metric_std = compute_metric_std(y_oot.values, oot_predictions, n_splits=3)\n",
    "\n",
    "        print(\"\\n--- OOT Hold-Out Results ---\")\n",
    "        print(f\"MAE: {mae_oot:.2f} ± {metric_std['MAE_std']:.2f}\")\n",
    "        print(f\"RMSE: {rmse_oot:.2f} ± {metric_std['RMSE_std']:.2f}\")\n",
    "        print(f\"R²: {r2_oot:.2f} ± {metric_std['R2_std']:.2f}\")\n",
    "        print(f\"Spearman Corr: {spearman_oot:.2f} ± {metric_std['Spearman_std']:.2f}\")\n",
    "\n",
    "        oot_results_df = pd.DataFrame({\n",
    "            'Metric': ['MAE', 'RMSE', 'R2', 'Spearman'],\n",
    "            'Mean': [mae_oot, rmse_oot, r2_oot, spearman_oot],\n",
    "            'Std': [\n",
    "                metric_std['MAE_std'],\n",
    "                metric_std['RMSE_std'],\n",
    "                metric_std['R2_std'],\n",
    "                metric_std['Spearman_std']\n",
    "            ]\n",
    "        })\n",
    "        oot_results_df.to_csv(metrics_output_filename, index=False, sep=';')\n",
    "\n",
    "        # 使用 tabulate 打印更漂亮的表格，不改变任何变量\n",
    "        from tabulate import tabulate\n",
    "        print(\"\\n=== OOT Hold-Out Summary (Mean ± Std) ===\")\n",
    "        print(tabulate(\n",
    "            oot_results_df,\n",
    "            headers=\"keys\",\n",
    "            tablefmt=\"psql\", \n",
    "            floatfmt=\".3f\"\n",
    "        ))\n",
    "\n",
    "        print(f\"\\nEvaluation summary saved to:\")\n",
    "        print(f\"  - Metrics CSV:      {metrics_output_filename}\")\n",
    "        print(f\"  - Rank metrics CSV: {rank_metrics_filename}\")\n",
    "        print(f\"  - Importance CSV:   {importance_output_filename}\")\n",
    "        if save_detailed_predictions:\n",
    "            detailed_output_filename = os.path.join(output_dir, f\"oot_predictions_detailed_{suffix}.csv\")\n",
    "            print(f\"  - Detailed preds:   {detailed_output_filename}\")\n",
    "\n",
    "\n",
    "        # Save simple predictions for visualization\n",
    "        oot_output_df = pd.DataFrame({'y_true': y_oot, 'y_pred': oot_predictions})\n",
    "        oot_output_df.to_csv(oot_predictions_output_filename, index=False, sep=';')\n",
    "\n",
    "        residuals = y_oot - oot_predictions\n",
    "        residuals_df = pd.DataFrame({\n",
    "            'Date': df_oot_target['Date'],\n",
    "            'y_true': y_oot,\n",
    "            'y_pred': oot_predictions,\n",
    "            'residual': residuals\n",
    "        })\n",
    "        residuals_path = os.path.join(output_dir, f\"residuals_{suffix}.csv\")\n",
    "        residuals_df.to_csv(residuals_path, sep=';', index=False)\n",
    "        print(f\"Residuals saved to '{residuals_path}'\")\n",
    "\n",
    "\n",
    "        # 预测置信区间 (95%)\n",
    "        residual_std = np.std(residuals)\n",
    "        ci_lower = oot_predictions - 1.96 * residual_std\n",
    "        ci_upper = oot_predictions + 1.96 * residual_std\n",
    "\n",
    "        ci_df = pd.DataFrame({\n",
    "            'y_pred': oot_predictions,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper\n",
    "        })\n",
    "        ci_path = os.path.join(output_dir, f\"confidence_interval_{suffix}.csv\")\n",
    "        ci_df.to_csv(ci_path, sep=';', index=False)\n",
    "        print(f\" Prediction confidence interval saved to '{ci_path}' (σ={residual_std:.3f})\")\n",
    "\n",
    "\n",
    "        # Save feature importance\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'importance': final_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "        feature_importance_df.to_csv(importance_output_filename, index=False, sep=';')\n",
    "\n",
    "        print(f\"Feature importance saved to '{importance_output_filename}'\")\n",
    "\n",
    "                #  Compute model-agnostic Permutation Feature Importance \n",
    "        try:\n",
    "            print(\"\\n=== Computing Permutation Feature Importance (Model-Agnostic) ===\")\n",
    "            perm_output_filename = os.path.join(output_dir, f\"importance_permutation_{suffix}.csv\")\n",
    "\n",
    "            # 仅在 OOT 集上计算，以避免过拟合影响\n",
    "            perm_result = permutation_importance(\n",
    "                final_model, X_oot, y_oot,\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                n_repeats=10,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            perm_importance_df = pd.DataFrame({\n",
    "                'feature': feature_columns,\n",
    "                'importance_mean': perm_result.importances_mean,\n",
    "                'importance_std': perm_result.importances_std\n",
    "            }).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "            perm_importance_df.to_csv(perm_output_filename, sep=';', index=False)\n",
    "            print(f\"Permutation importance saved to '{perm_output_filename}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Permutation importance computation failed: {e}\")\n",
    "\n",
    "        \n",
    "        # It saves the detailed, per-sample predictions needed for a paired t-test\n",
    "        if save_detailed_predictions:\n",
    "            # include the Date and true target for pairing\n",
    "            detailed_preds_df = df_oot_target[['Date', target_column]].copy()\n",
    "            detailed_preds_df['y_pred'] = oot_predictions\n",
    "            detailed_preds_df['model_name'] = model_name\n",
    "            \n",
    "            detailed_output_filename = os.path.join(output_dir, f\"oot_predictions_detailed_{suffix}.csv\")\n",
    "            detailed_preds_df.to_csv(detailed_output_filename, index=False, sep=';')\n",
    "            print(f\"Detailed predictions for significance testing saved to '{detailed_output_filename}'\")\n",
    "\n",
    "\n",
    "        print(f\" All results and data for {target_column} ({model_name}) saved to '{output_dir}'\")\n",
    "\n",
    "        return pd.DataFrame({'y_true': y_oot, 'y_pred': oot_predictions})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing for {target_column} ({model_name}): {e}\")\n",
    "\n",
    "\n",
    "def hyperparameter_search(X, y, is_classification=False):\n",
    "    print(f\"\\nStarting Hyperparameter Search (Mode: {'Classification' if is_classification else 'Regression'})\")\n",
    "    \n",
    "    param_dist = {\n",
    "        \"max_depth\": [6, 8, 10, 12],\n",
    "        \"num_leaves\": [32, 64, 128, 256],\n",
    "        \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "        \"n_estimators\": [300],\n",
    "        \"reg_lambda\": [3, 5, 7, 9],\n",
    "        \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "        \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "        \"random_state\": [42],\n",
    "    }\n",
    "    \n",
    "    N_ITER = 20  \n",
    "    N_SPLITS = 3\n",
    "    total_fits = N_ITER * N_SPLITS\n",
    "    device = 'gpu' if use_gpu else 'cpu'\n",
    "    \n",
    "    base_estimator = lgb.LGBMRegressor(\n",
    "        objective='regression',\n",
    "        metric='mae',\n",
    "        random_state=42,\n",
    "        n_jobs=1,  \n",
    "        device=device,\n",
    "        verbose=-1,\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Split for early stopping validation\n",
    "    split_idx = int(len(X) * 0.9)\n",
    "    X_search, X_val_search = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_search, y_val_search = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_estimator,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=N_ITER,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=TimeSeriesSplit(n_splits=N_SPLITS),\n",
    "        n_jobs=-1,  \n",
    "        random_state=42,\n",
    "        verbose=2,  \n",
    "    )\n",
    "\n",
    "    print(\"\\nFitting RandomizedSearchCV with early stopping...\")\n",
    "    callbacks = [\n",
    "        early_stopping(stopping_rounds=30),\n",
    "        log_evaluation(period=0)\n",
    "    ]\n",
    "\n",
    "    random_search.fit(\n",
    "        X_search, y_search,\n",
    "        eval_metric='mae',\n",
    "        eval_set=[(X_val_search, y_val_search)],  \n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(random_search.best_params_)\n",
    "    return random_search.best_params_\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Main Execution\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    # GPU configuration\n",
    "    os.environ[\"LIGHTGBM_USE_GPU\"] = \"1\"\n",
    "    os.environ[\"GPU_PLATFORM_ID\"] = \"0\"\n",
    "    os.environ[\"GPU_DEVICE_ID\"] = \"0\"\n",
    "    os.environ[\"LIGHTGBM_DEBUG_VERBOSE\"] = \"0\"\n",
    "    \n",
    "    # GPU Pre-flight Check\n",
    "    print(\"Step 0: Performing GPU Pre-flight Check...\")\n",
    "    try:\n",
    "        dummy_X = np.random.rand(10, 5)\n",
    "        dummy_y = np.random.rand(10)\n",
    "        dummy_model = lgb.LGBMRegressor(device='gpu')\n",
    "\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            dummy_model.fit(dummy_X, dummy_y)\n",
    "        print(\" GPU Pre-flight Check PASSED. LightGBM can access the GPU.\")\n",
    "        \n",
    "    except lgb.basic.LightGBMError as e:\n",
    "        print(\" GPU Pre-flight Check FAILED.\")\n",
    "        print(\"   Error:\", e)\n",
    "        print(\"   Please ensure LightGBM was installed with GPU support and that drivers are correct.\")\n",
    "        print(\"   The script will now exit.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during GPU check: {e}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    print(\"\\n\\n== RUNNING REGRESSION MODELS IN OOT VALIDATION MODE ===\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_filename, sep=';', parse_dates=['Date'])\n",
    "        df.sort_values('Date', inplace=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"CRITICAL ERROR: Input file '{input_filename}' not found. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # Add is_christmas feature if not present\n",
    "    if 'is_christmas' not in df.columns:\n",
    "        df['is_christmas'] = mark_christmas_period(df['Date'])\n",
    "        print(\"Added 'is_christmas' feature to dataset\")\n",
    "    else:\n",
    "        df['is_christmas'] = df['is_christmas'].astype(int)\n",
    "        print(\"'is_christmas' feature already present in dataset\")\n",
    "    \n",
    "    # Fill missing values in lag features\n",
    "    lag_fill_columns = [\n",
    "        'Rank_last_week', 'Points_last_week', 'Rank_change', 'Points_change',\n",
    "        'Points_rolling_mean_4w', 'Rank_rolling_mean_4w', 'Artist_Hotness'\n",
    "    ]\n",
    "    existing_fill = [col for col in lag_fill_columns if col in df.columns]\n",
    "    df[existing_fill] = df[existing_fill].fillna(0)\n",
    "    \n",
    "    # split train / val / test\n",
    "    train_df = df[df['Date'] <= TRAIN_END].copy()\n",
    "    val_df = df[(df['Date'] > TRAIN_END) & (df['Date'] <= VAL_END)].copy()\n",
    "    test_df = df[df['Date'] > VAL_END].copy()\n",
    "    \n",
    "\n",
    "    print(f\"  Train: <= {TRAIN_END.date()} ({train_df.shape[0]} rows)\")\n",
    "    print(f\"  Val:   {TRAIN_END.date()} < Date <= {VAL_END.date()} ({val_df.shape[0]} rows)\")\n",
    "    print(f\"  Test:  > {VAL_END.date()} ({test_df.shape[0]} rows)\")\n",
    "    print(f\"\\nChristmas weeks distribution:\")\n",
    "    print(f\"  Train: {train_df['is_christmas'].sum()} / {len(train_df)} ({100*train_df['is_christmas'].sum()/len(train_df):.1f}%)\")\n",
    "    print(f\"  Val:   {val_df['is_christmas'].sum()} / {len(val_df)} ({100*val_df['is_christmas'].sum()/len(val_df):.1f}%)\")\n",
    "    print(f\"  Test:  {test_df['is_christmas'].sum()} / {len(test_df)} ({100*test_df['is_christmas'].sum()/len(test_df):.1f}%)\")\n",
    "    \n",
    "    # Define feature sets for comparison\n",
    "    feature_sets = {\n",
    "        'baseline': BASELINE_FEATURES,\n",
    "        'baseline_plus_christmas': CHRISTMAS_FEATURES,\n",
    "    }\n",
    "    \n",
    "    # Define regression targets\n",
    "    regression_targets = ['Points_next_week']\n",
    "    \n",
    "    # Cache cleaned datasets to avoid repeated dropna\n",
    "    cached_targets = {}\n",
    "    for target in regression_targets:\n",
    "        for feature_set_name, feature_cols in feature_sets.items():\n",
    "            subset = train_df.dropna(subset=[target])\n",
    "            if not subset.empty:\n",
    "                cache_key = f\"{target}_{feature_set_name}\"\n",
    "                cached_targets[cache_key] = (subset[feature_cols], subset[target])\n",
    "    print(f\"Cached {len(cached_targets)} training subsets\")\n",
    "    \n",
    "    # Loop\n",
    "    for target in regression_targets:\n",
    "        print(f\"\\n\\n===== Processing Regression Target: {target} =====\")\n",
    "        \n",
    "        for feature_set_name, feature_cols in feature_sets.items():\n",
    "            cache_key = f\"{target}_{feature_set_name}\"\n",
    "            \n",
    "            if cache_key not in cached_targets:\n",
    "                print(f\"\\nSkipping {target} with {feature_set_name}: No training data after dropna.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n{'─'*80}\")\n",
    "            print(f\"Feature Set: {feature_set_name}\")\n",
    "            print(f\"Features: {len(feature_cols)} ({feature_cols[-1] if feature_set_name == 'baseline_plus_christmas' else 'baseline only'})\")\n",
    "            print(f\"{'─'*80}\")\n",
    "            \n",
    "            X_all, y_all = cached_targets[cache_key]\n",
    "            \n",
    "            # Hyperparameter search\n",
    "            print(f\"\\n--- Hyperparameter Search for {feature_set_name} ---\")\n",
    "            best_params = hyperparameter_search(X_all, y_all, is_classification=False)\n",
    "            \n",
    "            # Add fixed seeds and GPU config\n",
    "            best_params['random_state'] = 42\n",
    "            best_params['device'] = 'gpu' if use_gpu else 'cpu'\n",
    "            best_params['bagging_seed'] = 42\n",
    "            best_params['feature_fraction_seed'] = 42\n",
    "            best_params['n_estimators'] = 1800\n",
    "            \n",
    "            # Train and evaluate model\n",
    "            train_regression_pipeline(\n",
    "                train_df,\n",
    "                val_df,\n",
    "                test_df,\n",
    "                feature_cols,\n",
    "                target,\n",
    "                best_params,\n",
    "                model_name=feature_set_name,\n",
    "                save_detailed_predictions=True,\n",
    "            )\n",
    "    \n",
    "    print(\"CHRISTMAS MODEL PIPELINE COMPLETE\")\n",
    "    print(\"\\nComparison between baseline and baseline_plus_christmas available for:\")\n",
    "    for target in regression_targets:\n",
    "        print(f\"  - {target}\")\n",
    "    print(\"\\nDetailed predictions saved for significance testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b2299e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
