{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "步骤1:开始以文本形式加载并使用csv模块解析\n",
      "成功加载并解析文件, 原始数据形状: (651936, 20)\n",
      "解析后的列名: ['Rank', 'Title', 'Artists', 'Date', 'Danceability', 'Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Valence', '# of Artist', 'Artist (Ind.)', '# of Nationality', 'Nationality', 'Continent', 'Points (Total)', 'Points (Ind for each Artist/Nat)', 'id', 'Song URL']\n",
      "\n",
      "步骤2:开始进行数据类型转换和清理\n",
      "数据类型转换完成\n",
      "\n",
      "步骤 3: 开始转换数据形状 (按周聚合)\n",
      "数据形状转换完成, 新数据形状: (464475, 20)\n",
      "\n",
      "步骤4:开始修正Loudness列\n",
      "Loudness列修正完成。\n",
      "\n",
      "步骤 5: 开始排序最终数据\n",
      "数据排序完成。\n",
      "\n",
      "清洗完成! 适合每周预测的数据已保存到 'Spotify_Weekly_Data_Cleaned.csv'\n",
      "\n",
      "最终数据预览 (前5行):\n",
      "             Date                      id  Rank              Title  \\\n",
      "464368 2023-05-29  3qQbCzHBycnDpGskqOWY0E   1.0    Ella Baila Sola   \n",
      "464471 2023-05-29  7ro0hRteUMfnOioTFI5TG1   2.0     WHERE SHE GOES   \n",
      "464341 2023-05-29  2UW7JaomAMuX9pZrjVpHAU   3.0    La Bebe - Remix   \n",
      "464456 2023-05-29  7FbrGaHYVDmfr7KoLIZnQ7   4.0  Cupid - Twin Ver.   \n",
      "464451 2023-05-29  6pD0ufEQq0xdHSsRbg9LBK   5.0          un x100to   \n",
      "\n",
      "                           Artists  Danceability  Energy  Loudness  \\\n",
      "464368  Eslabon Armado, Peso Pluma         0.668   0.758   -5176.0   \n",
      "464471                   Bad Bunny         0.652   0.800   -4019.0   \n",
      "464341       Yng Lvcas, Peso Pluma         0.812   0.479   -5678.0   \n",
      "464456                 FIFTY FIFTY         0.783   0.592   -8332.0   \n",
      "464451   Grupo Frontera, Bad Bunny         0.569   0.724   -4076.0   \n",
      "\n",
      "        Speechiness  Acousticness  ...  Valence  Points (Total)  \\\n",
      "464368        0.033         0.483  ...    0.834           200.0   \n",
      "464471        0.061         0.143  ...    0.234           199.0   \n",
      "464341        0.333         0.213  ...    0.559           198.0   \n",
      "464456        0.033         0.435  ...    0.726           197.0   \n",
      "464451        0.047         0.228  ...    0.562           196.0   \n",
      "\n",
      "        Points (Ind for each Artist/Nat)  \\\n",
      "464368                             100.0   \n",
      "464471                             199.0   \n",
      "464341                              99.0   \n",
      "464456                             197.0   \n",
      "464451                              98.0   \n",
      "\n",
      "                                                                Song URL  \\\n",
      "464368   https://open.spotify.com/track/3qQbCzHBycnDpGskqOWY0E,,,,,,,,,,   \n",
      "464471  https://open.spotify.com/track/7ro0hRteUMfnOioTFI5TG1,,,,,,,,,,,   \n",
      "464341   https://open.spotify.com/track/2UW7JaomAMuX9pZrjVpHAU,,,,,,,,,,   \n",
      "464456  https://open.spotify.com/track/7FbrGaHYVDmfr7KoLIZnQ7,,,,,,,,,,,   \n",
      "464451   https://open.spotify.com/track/6pD0ufEQq0xdHSsRbg9LBK,,,,,,,,,,   \n",
      "\n",
      "                       Artist (Ind.)            Nationality  \\\n",
      "464368  [Eslabon Armado, Peso Pluma]       [Mexico, Mexico]   \n",
      "464471                   [Bad Bunny]          [Puerto Rico]   \n",
      "464341       [Yng Lvcas, Peso Pluma]       [Mexico, Mexico]   \n",
      "464456                 [FIFTY FIFTY]          [South Korea]   \n",
      "464451   [Grupo Frontera, Bad Bunny]  [Mexico, Puerto Rico]   \n",
      "\n",
      "                             Continent Artist_Count  Nationality_Count  \\\n",
      "464368  [Latin-America, Latin-America]            2                  1   \n",
      "464471                 [Latin-America]            1                  1   \n",
      "464341  [Latin-America, Latin-America]            2                  1   \n",
      "464456                          [Asia]            1                  1   \n",
      "464451  [Latin-America, Latin-America]            2                  2   \n",
      "\n",
      "        Loudness_Corrected  \n",
      "464368              -5.176  \n",
      "464471              -4.019  \n",
      "464341              -5.678  \n",
      "464456              -8.332  \n",
      "464451              -4.076  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "\n",
    "# 原始数据\n",
    "input_filename = 'Spotify_Dataset_V3.csv'\n",
    "# 清洗后数据\n",
    "output_filename = 'Spotify_Weekly_Data_Cleaned.csv'\n",
    "\n",
    "\n",
    "def robust_clean_spotify_data(file_path):\n",
    "    \"\"\"\n",
    "    最终版清洗流程:\n",
    "    1. 使用 utf-8-sig 编码和 csv 模块进行健壮的解析.\n",
    "    2. 将干净的数据加载到 pandas.\n",
    "    3. 按 (歌曲, 周) 聚合.\n",
    "    4. 对结果进行排序.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #1: 解析 CSV 文件 \n",
    "        print(\"步骤1:开始以文本形式加载并使用csv模块解析\")\n",
    "        \n",
    "        parsed_rows = []\n",
    "        # 使用'utf-8-sig'来处理文件开头的BOM隐藏字符\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "            reader = csv.reader(f, delimiter=';', quotechar='\"')\n",
    "            for row in reader:\n",
    "                parsed_rows.append(row)\n",
    "\n",
    "        header = parsed_rows[0]\n",
    "        cleaned_header = [col.split(',')[0].strip() for col in header]\n",
    "\n",
    "        df = pd.DataFrame(parsed_rows[1:], columns=cleaned_header)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        \n",
    "        print(f\"成功加载并解析文件, 原始数据形状: {df.shape}\")\n",
    "        print(f\"解析后的列名: {df.columns.tolist()}\")\n",
    "\n",
    "        # 步骤2:数据类型转换和清理\n",
    "        print(\"\\n步骤2:开始进行数据类型转换和清理\")\n",
    "\n",
    "        df.rename(columns={'# of Artist': 'Artist_Rank_Str', '# of Nationality': 'Nationality_Rank_Str'}, inplace=True)\n",
    "        \n",
    "        if 'id' not in df.columns or 'Date' not in df.columns:\n",
    "            raise ValueError(\"关键列'id'或'Date'不在数据中\")\n",
    "\n",
    "        # 将 'Date' 列转换为 datetime 对象\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "        numeric_cols = ['Rank', 'Danceability', 'Energy', 'Loudness', 'Speechiness', \n",
    "                        'Acousticness', 'Instrumentalness', 'Valence', 'Points (Total)', \n",
    "                        'Points (Ind for each Artist/Nat)']\n",
    "        for col in numeric_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "        df.dropna(subset=['id', 'Date'], inplace=True)\n",
    "        print(\"数据类型转换完成\")\n",
    "        \n",
    "        #  步骤3:转换数据形状 (按周和歌曲ID聚合)\n",
    "        print(\"\\n步骤 3: 开始转换数据形状 (按周聚合)\")\n",
    "        \n",
    "        aggregation_rules = {\n",
    "            'Rank': 'first', 'Title': 'first', 'Artists': 'first',\n",
    "            'Danceability': 'first', 'Energy': 'first', 'Loudness': 'first', 'Speechiness': 'first',\n",
    "            'Acousticness': 'first', 'Instrumentalness': 'first', 'Valence': 'first',\n",
    "            'Points (Total)': 'first', 'Points (Ind for each Artist/Nat)': 'first', 'Song URL': 'first',\n",
    "            'Artist (Ind.)': lambda x: list(x),\n",
    "            'Nationality': lambda x: list(x),\n",
    "            'Continent': lambda x: list(x),\n",
    "        }\n",
    "\n",
    "        # groupby自动按日期排序,\n",
    "        df_weekly = df.groupby(['Date', 'id']).agg(aggregation_rules).reset_index()\n",
    "\n",
    "        artist_counts = df.groupby(['Date', 'id']).size().reset_index(name='Artist_Count')\n",
    "        nationality_counts = df.groupby(['Date', 'id'])['Nationality'].nunique().reset_index(name='Nationality_Count')\n",
    "\n",
    "        df_weekly = pd.merge(df_weekly, artist_counts, on=['Date', 'id'])\n",
    "        df_weekly = pd.merge(df_weekly, nationality_counts, on=['Date', 'id'])\n",
    "        \n",
    "        print(f\"数据形状转换完成, 新数据形状: {df_weekly.shape}\")\n",
    "\n",
    "        #步骤 :清洗 Loudness列\n",
    "        print(\"\\n步骤4:开始修正Loudness列\")\n",
    "\n",
    "        def correct_loudness(loudness):\n",
    "            if pd.isna(loudness): return None\n",
    "            if loudness < -100: return loudness / 1000.0\n",
    "            return loudness\n",
    "\n",
    "        df_weekly['Loudness_Corrected'] = df_weekly['Loudness'].apply(correct_loudness)\n",
    "        print(\"Loudness列修正完成。\")\n",
    "        \n",
    "        # 进行最终排序数据\n",
    "        print(\"\\n步骤 5: 开始排序最终数据\")\n",
    "        # 首先按日期降序 (新日期在前), 然后在同一日期内按排名升序 (Rank 1 在前)\n",
    "        df_weekly.sort_values(by=['Date', 'Rank'], ascending=[False, True], inplace=True)\n",
    "        print(\"数据排序完成。\")\n",
    "\n",
    "        #保存数据 \n",
    "        df_weekly.to_csv(output_filename, index=False, sep=';', encoding='utf-8-sig')\n",
    "        print(f\"\\n清洗完成! 适合每周预测的数据已保存到 '{output_filename}'\")\n",
    "        \n",
    "        return df_weekly\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件 '{file_path}' 未找到。\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"处理过程中发生了一个错误: {e}\")\n",
    "        return None\n",
    "\n",
    "#主程序入口 \n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_data = robust_clean_spotify_data(input_filename)\n",
    "    if cleaned_data is not None:\n",
    "        print(\"\\n最终数据预览 (前5行):\")\n",
    "        pd.set_option('display.max_colwidth', 100)\n",
    "        print(cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1:Loading weekly aggregated data\n",
      "File loaded successfully. Shape: (464475, 21)\n",
      "\n",
      "Step2:Creating advanced features\n",
      "Successfully created 'Weeks_on_chart' feature.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG666\\AppData\\Local\\Temp\\ipykernel_49716\\44018391.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  artist_df['artist_hist_avg'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 'Artist_Hotness' feature.\n",
      "\n",
      " Step 3: Creating basic time-series features\n",
      "Successfully created lag, difference, and rolling features.\n",
      "\n",
      " Step4:Creating the target variable (next week's points) \n",
      "Target variable 'Points_next_week' created successfully.\n",
      "\n",
      "Step5:Cleaning missing values and selecting final features\n",
      "Final feature selection is complete.\n",
      "Shape of the model-ready dataset: (455363, 26)\n",
      "\n",
      "Feature engineering complete! Model-ready data saved to 'Spotify_Model_Ready_Features_V2.csv'\n",
      "\n",
      "Final data preview (first 5 rows):\n",
      "        Date                      id                                 Title  \\\n",
      "0 2017-03-24  000xQL6tZNLJzIrtIgxqSl  Still Got Time (feat. PARTYNEXTDOOR)   \n",
      "1 2017-03-25  000xQL6tZNLJzIrtIgxqSl  Still Got Time (feat. PARTYNEXTDOOR)   \n",
      "2 2017-03-26  000xQL6tZNLJzIrtIgxqSl  Still Got Time (feat. PARTYNEXTDOOR)   \n",
      "3 2017-03-27  000xQL6tZNLJzIrtIgxqSl  Still Got Time (feat. PARTYNEXTDOOR)   \n",
      "4 2017-03-28  000xQL6tZNLJzIrtIgxqSl  Still Got Time (feat. PARTYNEXTDOOR)   \n",
      "\n",
      "  Artists  Danceability  Energy  Loudness_Corrected  Speechiness  \\\n",
      "0    ZAYN         0.748   0.627              -6.029        0.064   \n",
      "1    ZAYN         0.748   0.627              -6.029        0.064   \n",
      "2    ZAYN         0.748   0.627              -6.029        0.064   \n",
      "3    ZAYN         0.748   0.627              -6.029        0.064   \n",
      "4    ZAYN         0.748   0.627              -6.029        0.064   \n",
      "\n",
      "   Acousticness  Instrumentalness  Valence  Artist_Count  Nationality_Count  \\\n",
      "0         0.131               0.0    0.524             1                  1   \n",
      "1         0.131               0.0    0.524             1                  1   \n",
      "2         0.131               0.0    0.524             1                  1   \n",
      "3         0.131               0.0    0.524             1                  1   \n",
      "4         0.131               0.0    0.524             1                  1   \n",
      "\n",
      "   Rank  Points (Total)  Rank_last_week  Points_last_week  Points_next_2weeks  \\\n",
      "0  50.0           151.0             0.0               0.0               134.0   \n",
      "1  68.0           133.0            50.0             151.0               153.0   \n",
      "2  67.0           134.0            68.0             133.0               158.0   \n",
      "3  48.0           153.0            67.0             134.0               161.0   \n",
      "4  43.0           158.0            48.0             153.0               160.0   \n",
      "\n",
      "   Points_next_4weeks  Rank_change  Points_change  Points_rolling_mean_4w  \\\n",
      "0               158.0          0.0            0.0                0.000000   \n",
      "1               161.0        -18.0          -18.0              151.000000   \n",
      "2               160.0          1.0            1.0              142.000000   \n",
      "3               154.0         19.0           19.0              139.333333   \n",
      "4               152.0          5.0            5.0              142.750000   \n",
      "\n",
      "   Rank_rolling_mean_4w  Weeks_on_chart  Artist_Hotness  Points_next_week  \n",
      "0              0.000000               1       29.697368             133.0  \n",
      "1             50.000000               2       31.272727             134.0  \n",
      "2             59.000000               3       32.576923             153.0  \n",
      "3             61.666667               4       33.860759             158.0  \n",
      "4             58.250000               5       35.350000             161.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "input_filename = 'Spotify_Weekly_Data_Cleaned.csv'\n",
    "output_filename = 'Spotify_Model_Ready_Features_V2.csv'\n",
    "\n",
    "def create_advanced_features(file_path):\n",
    "    try:\n",
    "        print(\"Step1:Loading weekly aggregated data\")\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        \n",
    "        #确保日期列是日期时间对象\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        print(f\"File loaded successfully. Shape: {df.shape}\")\n",
    "        print(\"\\nStep2:Creating advanced features\")\n",
    "\n",
    "        #必须先按歌曲ID分组再按日期升序排序\n",
    "        df.sort_values(by=['id', 'Date'], ascending=[True, True], inplace=True)\n",
    "        \n",
    "        # 第一个新特征创建歌曲生命周期特征上榜周数\n",
    "        # cumcount函数是从零开始的累积计数加一后即为上榜周数\n",
    "        df['Weeks_on_chart'] = df.groupby('id').cumcount() + 1\n",
    "        print(\"Successfully created 'Weeks_on_chart' feature.\")\n",
    "\n",
    "        # 第二个新特征创建艺人热度特征这是一个多步骤的过程\n",
    "        # 检查并转换艺人独立列\n",
    "        # CSV读取时列表会变成字符串需要用此函数安全地转换回来\n",
    "        if isinstance(df['Artist (Ind.)'].iloc[0], str):\n",
    "            df['Artist (Ind.)'] = df['Artist (Ind.)'].apply(ast.literal_eval)\n",
    "        \n",
    "        # 展开数据框让每一行代表一个艺人歌曲周的组合\n",
    "        artist_df = df[['Date', 'id', 'Artist (Ind.)', 'Points (Total)']].explode('Artist (Ind.)')\n",
    "\n",
    "        # 计算每个艺人截至当前日期的历史平均分不包含当前周防止数据泄露\n",
    "        artist_df.sort_values(['Artist (Ind.)', 'Date'], inplace=True)\n",
    "        artist_df['artist_hist_avg'] = artist_df.groupby('Artist (Ind.)')['Points (Total)'].transform(\n",
    "            lambda x: x.shift(1).expanding().mean()\n",
    "        )\n",
    "        # 对于艺人的首次出现历史平均分为空值用零填充\n",
    "        artist_df['artist_hist_avg'].fillna(0, inplace=True)\n",
    "\n",
    "        # 聚合得到每首歌在每一周的最高艺人热度\n",
    "        # 即合作艺人中历史表现最好的那个人的分数\n",
    "        artist_hotness = artist_df.groupby(['Date', 'id'])['artist_hist_avg'].max().reset_index(name='Artist_Hotness')\n",
    "\n",
    "        # 将计算出的热度特征合并回主数据框\n",
    "        df = pd.merge(df, artist_hotness, on=['Date', 'id'], how='left')\n",
    "        print(\"Successfully created 'Artist_Hotness' feature.\")\n",
    "        print(\"\\n Step 3: Creating basic time-series features\")\n",
    "        \n",
    "        #按ID分组\n",
    "        grouped = df.groupby('id')\n",
    "        \n",
    "        #创建滞后特征即上一周的表现\n",
    "        df['Rank_last_week'] = grouped['Rank'].shift(1)\n",
    "        df['Points_last_week'] = grouped['Points (Total)'].shift(1)\n",
    "        \n",
    "        #创建变化量特征\n",
    "        df['Rank_change'] = df['Rank_last_week'] - df['Rank'] #正数表示排名上升\n",
    "        df['Points_change'] = df['Points (Total)'] - df['Points_last_week']\n",
    "        \n",
    "        #创建滚动特征即最近四周的平均表现\n",
    "        df['Points_rolling_mean_4w'] = grouped['Points (Total)'].transform(lambda x: x.shift(1).rolling(window=4, min_periods=1).mean())\n",
    "        df['Rank_rolling_mean_4w'] = grouped['Rank'].transform(lambda x: x.shift(1).rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        print(\"Successfully created lag, difference, and rolling features.\")\n",
    "\n",
    "\n",
    "        print(\"\\n Step4:Creating the target variable (next week's points) \")\n",
    "        #目标是预测下n周的积分所以我们将积分向上移动n行\n",
    "        df['Points_next_week'] = grouped['Points (Total)'].shift(-1)\n",
    "        df['Points_next_2weeks'] = grouped['Points (Total)'].shift(-2)\n",
    "        df['Points_next_4weeks'] = grouped['Points (Total)'].shift(-4)\n",
    "        print(\"Target variable 'Points_next_week' created successfully.\")\n",
    "        \n",
    "        # --- 必要修改 1: 新增排名升降目标变量 ---\n",
    "        df['Rank_next_week'] = grouped['Rank'].shift(-1)\n",
    "        rank_change_next_week = df['Rank'] - df['Rank_next_week']\n",
    "        conditions = [\n",
    "            rank_change_next_week > 0,\n",
    "            rank_change_next_week < 0\n",
    "        ]\n",
    "        choices = ['Rise', 'Fall']\n",
    "        df['Rank_change_direction_next_week'] = np.select(conditions, choices, default='Stable')\n",
    "\n",
    "\n",
    "        print(\"\\nStep5:Cleaning missing values and selecting final features\")\n",
    "        \n",
    "        # 对于一首歌第一次上榜的记录它的历史特征是空值\n",
    "        # 用零来填充这可以作为模型识别新上榜的信号\n",
    "        feature_cols_to_fill = [\n",
    "            'Rank_last_week', 'Points_last_week', 'Rank_change', 'Points_change',\n",
    "            'Points_rolling_mean_4w', 'Rank_rolling_mean_4w', 'Artist_Hotness'\n",
    "        ]\n",
    "        df[feature_cols_to_fill] = df[feature_cols_to_fill].fillna(0)\n",
    "        \n",
    "        # df.dropna(subset=['Points_next_week'], inplace=True)\n",
    "        \n",
    "        # 选择最终特征集\n",
    "        final_features = [\n",
    "            # 核心音频特征\n",
    "            'Danceability', 'Energy', 'Loudness_Corrected', 'Speechiness', \n",
    "            'Acousticness', 'Instrumentalness', 'Valence',\n",
    "            # 合作相关特征\n",
    "            'Artist_Count', 'Nationality_Count',\n",
    "            # 当前状态特征\n",
    "            'Rank', 'Points (Total)',\n",
    "            # 创建历史特征\n",
    "            'Rank_last_week', 'Points_last_week',\n",
    "            'Rank_change', 'Points_change', 'Points_rolling_mean_4w', 'Rank_rolling_mean_4w',\n",
    "            'Weeks_on_chart', 'Artist_Hotness',\n",
    "            # 目标变量\n",
    "            'Points_next_week','Points_next_2weeks', 'Points_next_4weeks',\n",
    "            'Rank_change_direction_next_week'\n",
    "        ]\n",
    "        \n",
    "        # 加入上下文信息列以便后续按时间划分数据集\n",
    "        context_features = ['Date', 'id', 'Title', 'Artists']\n",
    "        final_df = df[context_features + final_features].copy()\n",
    "\n",
    "        print(\"Final feature selection is complete.\")\n",
    "        print(f\"Shape of the model-ready dataset: {final_df.shape}\")\n",
    "        \n",
    "        #Save Data\n",
    "        final_df.to_csv(output_filename, index=False, sep=';', encoding='utf-8-sig')\n",
    "        print(f\"\\nFeature engineering complete! Model-ready data saved to '{output_filename}'\")\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found. Please ensure the previous step ran successfully.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution block \n",
    "if __name__ == \"__main__\":\n",
    "    model_ready_data = create_advanced_features(input_filename)\n",
    "    if model_ready_data is not None:\n",
    "        print(\"\\nFinal data preview (first 5 rows):\")\n",
    "        pd.set_option('display.max_columns', None) # 显示所有列\n",
    "        print(model_ready_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation for RQ4: From Missing Values to Chart (Re-)Entry Prediction\n",
    "\n",
    "在构建特征时，我们为每首歌曲生成了其上一周的相关特征，例如：\n",
    "\n",
    "```python\n",
    "df['Rank_last_week'] = grouped['Rank'].shift(1)\n",
    "df['Points_last_week'] = grouped['Points (Total)'].shift(1)\n",
    "\n",
    "在这一过程中，我们注意到：如果某首歌在下一周未再出现在榜单中（即下榜），其对应的 Points_next_week 将被设置为缺失值并最终被剔除。\n",
    "这意味着模型在 RQ1 的训练中，只能学习那些“连续上榜”的样本，而未能考虑“下榜”或“重新上榜”的动态行为。\n",
    "\n",
    "基于这一发现，我们进一步提出了新的研究任务 —— 上下榜预测（Drop & Re-entry Prediction）。\n",
    "这一扩展的任务旨在弥补 RQ1 在逻辑与任务本质上的局限，使模型不仅能够预测积分变化趋势，还能识别歌曲热度的转折点：\n",
    "\n",
    "下榜预测（Drop Prediction）：判断当前上榜歌曲是否将在下一周退出榜单；\n",
    "\n",
    "上榜预测（Re-entry Prediction）：判断当前未上榜的歌曲是否会在下一周重新进入榜单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "== RUNNING IN HYPERPARAMETER SEARCH MODE ===\n",
      "\n",
      " (Optional) Starting Hyperparameter Search\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3932\n",
      "[LightGBM] [Info] Number of data points in the train set: 455363, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 100.749850\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.9, 'num_leaves': 50, 'n_estimators': 1000, 'max_depth': 30, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "\n",
      "\n",
      "===== Training model for target: Points_next_week =====\n",
      "Step 1: Loading model-ready data\n",
      "File loaded and sorted by date. Shape: (455363, 24)\n",
      "\n",
      "Step 2: Defining features and target\n",
      "\n",
      "Step 3: Performing Time Series Cross-Validation\n",
      "Using model parameters: {'subsample': 0.9, 'num_leaves': 50, 'n_estimators': 1000, 'max_depth': 30, 'learning_rate': 0.01, 'colsample_bytree': 0.8, 'random_state': 42}\n",
      "Fold 1/5\n",
      "Training on 75898 samples from 2017-01-01 to 2018-02-01\n",
      "Validating on 75893 samples from 2018-02-01 to 2019-02-26\n",
      "An error occurred during processing: fit() got an unexpected keyword argument 'verbose'\n",
      "\n",
      "\n",
      "===== Training model for target: Points_next_2weeks =====\n",
      "Step 1: Loading model-ready data\n",
      "File loaded and sorted by date. Shape: (455363, 24)\n",
      "\n",
      "Step 2: Defining features and target\n",
      "An error occurred during processing: 'Points_next_2weeks'\n",
      "\n",
      "\n",
      "===== Training model for target: Points_next_4weeks =====\n",
      "Step 1: Loading model-ready data\n",
      "File loaded and sorted by date. Shape: (455363, 24)\n",
      "\n",
      "Step 2: Defining features and target\n",
      "An error occurred during processing: 'Points_next_4weeks'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, precision_score, recall_score\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # 导入seaborn库以美化图表\n",
    "import numpy as np\n",
    "import joblib\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "#  Configurations\n",
    "input_filename = 'Spotify_Model_Ready_Features_V2.csv'\n",
    "metrics_output_filename = 'model_performance_metrics_cv.csv'\n",
    "importance_output_filename = 'feature_importance_final_model.csv'\n",
    "model_output_filename = 'lgbm_spotify_model.pkl'\n",
    "\n",
    "def train_with_time_series_cv(file_path, model_params=None, target_column='Points_next_week'):\n",
    "    try:\n",
    "        sns.set_theme(style=\"whitegrid\", palette=\"viridis\", font_scale=1.1)\n",
    "        \n",
    "        print(\"Step 1: Loading model-ready data\")\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.sort_values('Date', inplace=True)\n",
    "        print(f\"File loaded and sorted by date. Shape: {df.shape}\")\n",
    "\n",
    "        # 自动区分不同预测周期的输出文件名\n",
    "        suffix = target_column.replace('Points_', '')\n",
    "        metrics_output_filename = f\"metrics_{suffix}.csv\"\n",
    "        importance_output_filename = f\"importance_{suffix}.csv\"\n",
    "        model_output_filename = f\"model_{suffix}.pkl\"\n",
    "\n",
    "        #  Define Features and Target \n",
    "        print(\"\\nStep 2: Defining features and target\")\n",
    "\n",
    "        feature_columns = [\n",
    "            'Danceability', 'Energy', 'Loudness_Corrected', 'Speechiness', \n",
    "            'Acousticness', 'Instrumentalness', 'Valence', 'Artist_Count', \n",
    "            'Nationality_Count', 'Rank', 'Points (Total)', 'Rank_last_week', \n",
    "            'Points_last_week', 'Rank_change', 'Points_change', \n",
    "            'Points_rolling_mean_4w', 'Rank_rolling_mean_4w', \n",
    "            'Weeks_on_chart', 'Artist_Hotness'\n",
    "        ]\n",
    "\n",
    "        X = df[feature_columns]\n",
    "        y = df[target_column]\n",
    "\n",
    "        # Time Series Cross-Validation\n",
    "        print(\"\\nStep 3: Performing Time Series Cross-Validation\")\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        mae_scores, r2_scores, spearman_scores = [], [], []\n",
    "\n",
    "        # 我们只在最后一次分割上进行可视化，以避免生成过多图片\n",
    "        last_val_indices = None\n",
    "\n",
    "        # 如果没有指定参数，就用默认参数\n",
    "        if model_params is None:\n",
    "            model_params = {'random_state': 42, 'n_estimators': 500, 'learning_rate': 0.05, 'num_leaves': 31}\n",
    "        \n",
    "        print(f\"Using model parameters: {model_params}\")\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "            last_val_indices = val_index # 持续更新，最后一次循环时即为最终测试集\n",
    "            print(f\"Fold {fold+1}/5\")\n",
    "            X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "            \n",
    "            train_dates = df.iloc[train_index]['Date']\n",
    "            val_dates = df.iloc[val_index]['Date']\n",
    "            print(f\"Training on {len(X_train)} samples from {train_dates.min().date()} to {train_dates.max().date()}\")\n",
    "            print(f\"Validating on {len(X_val)} samples from {val_dates.min().date()} to {val_dates.max().date()}\")\n",
    "\n",
    "            lgbm = lgb.LGBMRegressor(**model_params, device='gpu')\n",
    "            lgbm.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric='l1',\n",
    "                callbacks=[early_stopping(stopping_rounds=50, verbose=False)]\n",
    "            )\n",
    "            \n",
    "            predictions = lgbm.predict(X_val)\n",
    "            \n",
    "            mae = mean_absolute_error(y_val, predictions)\n",
    "            r2 = r2_score(y_val, predictions)\n",
    "            spearman_corr, _ = spearmanr(y_val, predictions)\n",
    "            \n",
    "            mae_scores.append(mae)\n",
    "            r2_scores.append(r2)\n",
    "            spearman_scores.append(spearman_corr)\n",
    "            print(f\"MAE: {mae:.2f}, R2: {r2:.2f}, Spearman Corr: {spearman_corr:.2f}\\n\")\n",
    "\n",
    "        print(\"\\nStep4:Aggregating Cross-Validation Result\")\n",
    "\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'Metric': ['Average MAE (5-fold CV)', 'Std Dev of MAE (5-fold CV)', 'Average R2 (5-fold CV)', 'Average Spearman Correlation'],\n",
    "            'Value': [np.mean(mae_scores), np.std(mae_scores), np.mean(r2_scores), np.mean(spearman_scores)],\n",
    "            'Description': [\n",
    "                '5次验证的平均绝对误差，这是模型性能的稳健估计',\n",
    "                '误差的标准差，衡量模型性能的稳定性，越小越好',\n",
    "                '5次验证的平均R平方，衡量模型的平均解释能力',\n",
    "                '预测排名与真实排名的一致性，越接近1越好'\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        print(\"\\nModel Performance (Cross-Validated)\")\n",
    "        print(metrics_df.to_string())\n",
    "        metrics_df.to_csv(metrics_output_filename, index=False, sep=';', encoding='utf-8-sig')\n",
    "        print(f\"\\nCross-validated metrics saved to '{metrics_output_filename}'\")\n",
    "\n",
    "        # Train Final Model and Analyze Importance \n",
    "        print(\"\\nStep 5 Training final model on ALL data for feature analysis\")\n",
    "    \n",
    "        final_model = lgb.LGBMRegressor(**model_params, device='gpu')\n",
    "        final_model.fit(X, y)\n",
    "        print(\"Final model training complete.\")\n",
    "\n",
    "        joblib.dump(final_model, model_output_filename)\n",
    "        print(f\"Final model saved to '{model_output_filename}'\")\n",
    "        \n",
    "        #Step 6: Final Evaluation and Plotting\n",
    "        print(\"\\n Step 6: Final evaluation on a hold-out set and plotting\")\n",
    "        \n",
    "        #使用最后一次交叉验证的验证集作为我们的最终测试集，以保持一致性\n",
    "        test_df = df.iloc[last_val_indices].copy()\n",
    "        final_predictions = final_model.predict(test_df[feature_columns])\n",
    "        y_test = test_df[target_column]\n",
    "\n",
    "        #1.特征重要性图 (Figure 2)\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'importance': final_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "        feature_importance_df.to_csv(importance_output_filename, index=False, sep=';', encoding='utf-8-sig')\n",
    "        print(f\"\\nFinal model's feature importances saved to '{importance_output_filename}'\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.head(15), palette=\"viridis\")\n",
    "        plt.title(\"Top 15 Feature Importances (Final Model)\", fontsize=18, weight='bold')\n",
    "        plt.xlabel(\"LightGBM Feature Importance\", fontsize=14)\n",
    "        plt.ylabel(\"Feature\", fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        plt.close()\n",
    "        print(\"Feature importance plot saved as 'feature_importance.png'\")\n",
    "        \n",
    "        # 2.真实值 vs 预测值散点图 (Figure 3)\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        sns.scatterplot(x=y_test, y=final_predictions, alpha=0.5, ax=ax, edgecolor='k', s=80)\n",
    "        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "        ax.set_xlabel(\"Actual Points (Next Week)\", fontsize=14)\n",
    "        ax.set_ylabel(\"Predicted Points (Next Week)\", fontsize=14)\n",
    "        ax.set_title(\"Actual vs. Predicted Points on Test Set\", fontsize=16, weight='bold')\n",
    "        ax.legend()\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('actual_vs_predicted.png')\n",
    "        plt.close()\n",
    "        print(\"Actual vs. Predicted plot saved as 'actual_vs_predicted.png'\")\n",
    "\n",
    "        # 3.残差图 (Figure 5)\n",
    "        residuals = y_test - final_predictions\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=final_predictions, y=residuals, alpha=0.5, edgecolor='k', s=80)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel(\"Predicted Points\", fontsize=14)\n",
    "        plt.ylabel(\"Residuals (Actual - Predicted)\", fontsize=14)\n",
    "        plt.title(\"Residual Plot\", fontsize=16, weight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('residuals_plot.png')\n",
    "        plt.close()\n",
    "        print(\"Residual plot saved as 'residuals_plot.png'\")\n",
    "        \n",
    "        # 4.预测误差分布图 (Figure 6)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(residuals, kde=True, bins=50)\n",
    "        plt.title('Distribution of Prediction Errors (Residuals)', fontsize=16, weight='bold')\n",
    "        plt.xlabel('Prediction Error (Actual - Predicted)', fontsize=14)\n",
    "        plt.ylabel('Frequency', fontsize=14)\n",
    "        plt.axvline(x=residuals.mean(), color='r', linestyle='--', label=f'Mean Error: {residuals.mean():.2f}')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('prediction_error_distribution.png')\n",
    "        plt.close()\n",
    "        print(\"Prediction error distribution plot saved as 'prediction_error_distribution.png'\")\n",
    "\n",
    "                # --- Step 7: Out-of-Time (OOT) Hold-Out Testing ---\n",
    "        print(\"\\n Step 7: Performing Out-of-Time (OOT) Hold-Out Testing\")\n",
    "\n",
    "        # 划分最近 3 个月的数据作为未来未见的测试集\n",
    "        split_date = df['Date'].max() - pd.DateOffset(months=3)\n",
    "        oot_df = df[df['Date'] >= split_date].copy()\n",
    "        X_oot = oot_df[feature_columns]\n",
    "        y_oot = oot_df[target_column]\n",
    "\n",
    "        oot_predictions = final_model.predict(X_oot)\n",
    "\n",
    "        # 计算指标\n",
    "        mae_oot = mean_absolute_error(y_oot, oot_predictions)\n",
    "        r2_oot = r2_score(y_oot, oot_predictions)\n",
    "        spearman_oot, _ = spearmanr(y_oot, oot_predictions)\n",
    "\n",
    "        print(\"\\n--- OOT Hold-Out Results ---\")\n",
    "        print(f\"Time Range: {oot_df['Date'].min().date()} → {oot_df['Date'].max().date()}\")\n",
    "        print(f\"Samples: {len(oot_df)}\")\n",
    "        print(f\"MAE: {mae_oot:.2f}\")\n",
    "        print(f\"R²: {r2_oot:.2f}\")\n",
    "        print(f\"Spearman Corr: {spearman_oot:.2f}\")\n",
    "\n",
    "        # 保存结果到文件\n",
    "        oot_results_df = pd.DataFrame({\n",
    "            'Metric': ['MAE', 'R2', 'Spearman'],\n",
    "            'Value': [mae_oot, r2_oot, spearman_oot],\n",
    "            'Description': [\n",
    "                '绝对误差（越低越好）',\n",
    "                '解释方差（越高越好）',\n",
    "                '预测与真实的排名一致性（越高越好）'\n",
    "            ]\n",
    "        })\n",
    "        oot_results_df.to_csv('oot_holdout_results.csv', index=False, sep=';', encoding='utf-8-sig')\n",
    "        print(\"OOT metrics saved to 'oot_holdout_results.csv'\")\n",
    "\n",
    "        # 绘制 OOT 测试结果散点图\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        sns.scatterplot(x=y_oot, y=oot_predictions, alpha=0.5, edgecolor='k', s=80)\n",
    "        plt.plot([y_oot.min(), y_oot.max()], [y_oot.min(), y_oot.max()], 'r--', lw=2)\n",
    "        plt.title('OOT Test: Actual vs Predicted (Unseen Future Period)', fontsize=16, weight='bold')\n",
    "        plt.xlabel('Actual Points (Next Week)', fontsize=14)\n",
    "        plt.ylabel('Predicted Points (Next Week)', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('oot_actual_vs_predicted.png')\n",
    "        plt.close()\n",
    "        print(\"OOT actual vs predicted plot saved as 'oot_actual_vs_predicted.png'\")\n",
    "\n",
    "\n",
    "        results = lgbm.evals_result_\n",
    "        plt.plot(results['valid_0']['l1'])\n",
    "        plt.title(f'Fold {fold+1} Validation MAE over Iterations')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'learning_curve_fold_{fold+1}.png')\n",
    "        plt.close()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")\n",
    "\n",
    "def hyperparameter_search(X, y):\n",
    "    \"\"\"\n",
    "    (可选) 使用随机搜索寻找最佳超参数。\n",
    "    \"\"\"\n",
    "    print(\"\\n (Optional) Starting Hyperparameter Search\")\n",
    "    param_dist = {\n",
    "        'n_estimators': [500, 1000, 1500, 2000],\n",
    "        'learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "        'num_leaves': [31, 50, 70, 100],\n",
    "        'max_depth': [-1, 10, 20, 30],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    lgbm = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=lgbm, param_distributions=param_dist, n_iter=25,\n",
    "        scoring='neg_mean_absolute_error', cv=tscv, n_jobs=-1,\n",
    "        verbose=2, random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(X, y)\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(random_search.best_params_)\n",
    "    return random_search.best_params_\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"\\n\\n== RUNNING IN HYPERPARAMETER SEARCH MODE ===\")\n",
    "    df = pd.read_csv(input_filename, sep=';')\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.sort_values('Date', inplace=True)\n",
    "\n",
    "    feature_columns = [\n",
    "        'Danceability', 'Energy', 'Loudness_Corrected', 'Speechiness', \n",
    "        'Acousticness', 'Instrumentalness', 'Valence', 'Artist_Count', \n",
    "        'Nationality_Count', 'Rank', 'Points (Total)', 'Rank_last_week', \n",
    "        'Points_last_week', 'Rank_change', 'Points_change', \n",
    "        'Points_rolling_mean_4w', 'Rank_rolling_mean_4w', \n",
    "        'Weeks_on_chart', 'Artist_Hotness'\n",
    "    ]\n",
    "\n",
    "    target_columns = [\n",
    "        'Points_next_week',\n",
    "        'Points_next_2weeks',\n",
    "        'Points_next_4weeks'\n",
    "    ]\n",
    "\n",
    "    short_term_df = df.dropna(subset=['Points_next_week'])\n",
    "    X_all = short_term_df[feature_columns]\n",
    "    y_all = short_term_df['Points_next_week']\n",
    "    best_params = hyperparameter_search(X_all, y_all)\n",
    "    best_params['random_state'] = 42\n",
    "\n",
    "    for target in target_columns:\n",
    "        print(f\"\\n\\n===== Training model for target: {target} =====\")\n",
    "        train_with_time_series_cv(input_filename, model_params=best_params, target_column=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading final model and data\n",
      "Model and data loaded successfully. Data shape: (455363, 24)\n",
      "\n",
      "Step 2: Defining test set and making predictions\n",
      "\n",
      "Step 3: Defining granular groups for analysis\n",
      "Groups defined based on 'start_type'.\n",
      "\n",
      "Group Counts:\n",
      "start_type\n",
      "Hot Start (Old Song)                         17458\n",
      "Warm Start (New Song, Established Artist)      170\n",
      "True Cold Start (New Song, New Artist)          23\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Step 4: Calculating performance metrics for each group \n",
      "\n",
      "Step 5: Aggregating and saving results\n",
      "\n",
      "RQ2 Performance Comparison (Advanced)\n",
      "                                       Group  Sample_Size        MAE  R2_Score  Spearman_Correlation\n",
      "0                       Hot Start (Old Song)        17458   7.275243  0.961700              0.980375\n",
      "2  Warm Start (New Song, Established Artist)          170  23.821281  0.716141              0.841260\n",
      "1     True Cold Start (New Song, New Artist)           23  34.008921 -0.019694              0.363726\n",
      "\n",
      "RQ2 analysis results saved to 'rq2_performance_comparison_advanced.csv'\n",
      "\n",
      "Step 6: Generating comparison plot\n",
      "Comparison plot saved to 'rq2_mae_comparison_advanced.png'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "model_filename = 'lgbm_spotify_model.pkl'\n",
    "data_filename = 'Spotify_Model_Ready_Features_V2.csv'\n",
    "output_metrics_filename = 'rq2_performance_comparison_advanced.csv'\n",
    "output_plot_filename = 'rq2_mae_comparison_advanced.png'\n",
    "\n",
    "def analyze_rq2_advanced(model_path, data_path):\n",
    "    \"\"\"\n",
    "    执行RQ2的深度分析：比较模型在不同启动类型（冷/温/热启动）上的表现。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Step 1: Loading final model and data\")\n",
    "        model = joblib.load(model_path)\n",
    "        df = pd.read_csv(data_path, sep=';')\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        print(f\"Model and data loaded successfully. Data shape: {df.shape}\")\n",
    "\n",
    "        print(\"\\nStep 2: Defining test set and making predictions\")\n",
    "        split_date = df['Date'].max() - pd.DateOffset(months=3)\n",
    "        test_df = df[df['Date'] >= split_date].copy()\n",
    "        \n",
    "        feature_columns = [\n",
    "            'Danceability', 'Energy', 'Loudness_Corrected', 'Speechiness', \n",
    "            'Acousticness', 'Instrumentalness', 'Valence', 'Artist_Count', \n",
    "            'Nationality_Count', 'Rank', 'Points (Total)', 'Rank_last_week', \n",
    "            'Points_last_week', 'Rank_change', 'Points_change', \n",
    "            'Points_rolling_mean_4w', 'Rank_rolling_mean_4w', \n",
    "            'Weeks_on_chart', 'Artist_Hotness'\n",
    "        ]\n",
    "        target_column = 'Points_next_week'\n",
    "\n",
    "        X_test = test_df[feature_columns]\n",
    "        y_test = test_df[target_column]\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        test_df['predictions'] = predictions\n",
    "\n",
    "        print(\"\\nStep 3: Defining granular groups for analysis\")\n",
    "        \n",
    "        def define_start_type(row):\n",
    "            is_new_song = row['Weeks_on_chart'] == 1\n",
    "            is_new_artist = row['Artist_Hotness'] == 0\n",
    "            \n",
    "            if is_new_song and is_new_artist:\n",
    "                return 'True Cold Start (New Song, New Artist)'\n",
    "            elif is_new_song and not is_new_artist:\n",
    "                return 'Warm Start (New Song, Established Artist)'\n",
    "            else: # is_old_song\n",
    "                return 'Hot Start (Old Song)'\n",
    "\n",
    "        test_df['start_type'] = test_df.apply(define_start_type, axis=1)\n",
    "        \n",
    "        print(\"Groups defined based on 'start_type'.\")\n",
    "        print(\"\\nGroup Counts:\")\n",
    "        print(test_df['start_type'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\nStep 4: Calculating performance metrics for each group \")\n",
    "        results = []\n",
    "\n",
    "        def evaluate_group(group_df, group_name):\n",
    "            if len(group_df) == 0: return None\n",
    "            mae = mean_absolute_error(group_df[target_column], group_df['predictions'])\n",
    "            r2 = r2_score(group_df[target_column], group_df['predictions'])\n",
    "            spearman_corr, _ = spearmanr(group_df[target_column], group_df['predictions'])\n",
    "            return {\n",
    "                'Group': group_name,\n",
    "                'Sample_Size': len(group_df),\n",
    "                'MAE': mae,\n",
    "                'R2_Score': r2,\n",
    "                'Spearman_Correlation': spearman_corr\n",
    "            }\n",
    "        \n",
    "        # 按我们新定义的“启动类型”进行分组评估\n",
    "        for group_name, group_df in test_df.groupby('start_type'):\n",
    "            result = evaluate_group(group_df, group_name)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        \n",
    "        # 结果汇总\n",
    "        print(\"\\nStep 5: Aggregating and saving results\")\n",
    "        \n",
    "        results_df = pd.DataFrame(results).sort_values('MAE')\n",
    "        \n",
    "        print(\"\\nRQ2 Performance Comparison (Advanced)\")\n",
    "        print(results_df.to_string())\n",
    "        \n",
    "        results_df.to_csv(output_metrics_filename, index=False, sep=';', encoding='utf-8-sig')\n",
    "        print(f\"\\nRQ2 analysis results saved to '{output_metrics_filename}'\")\n",
    "\n",
    "        #可视化对比结果\n",
    "        print(\"\\nStep 6: Generating comparison plot\")\n",
    "        \n",
    "        sns.set_theme(style=\"whitegrid\", palette=\"viridis\", font_scale=1.1)\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        \n",
    "        sns.barplot(x='MAE', y='Group', data=results_df, orient='h')\n",
    "        \n",
    "        plt.title('Model Prediction Error (MAE) Comparison by Start Type', fontsize=18, weight='bold')\n",
    "        plt.xlabel('Mean Absolute Error (Lower is Better)', fontsize=14)\n",
    "        plt.ylabel('Group Type', fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_plot_filename)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Comparison plot saved to '{output_plot_filename}'\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Make sure '{model_path}' and '{data_path}' exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")\n",
    "\n",
    "# 主程序入口\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_rq2_advanced(model_filename, data_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型性能分析：热启动、温启动与冷启动\n",
    "\n",
    "在这一分析中，我们根据模型在不同情境下的表现，分别探讨了热启动（Hot Start）、温启动（Warm Start）和真·冷启动（True Cold Start）的情况。以下是三种情况下的详细表现及其解释。\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **热启动 (Hot Start / 老歌)** - **模型的主场优势**\n",
    "   - **MAE (平均误差)**: **7.28分** \n",
    "   - **R² (解释力)**: **0.96** -\n",
    "   - **Spearman (排名相关性)**: **0.98** -\n",
    "\n",
    "   **分析**：\n",
    "   当一首歌已经在榜单上并且有了足够的历史数据（例如 `Weeks_on_chart > 1`），模型的表现非常出色。在这种情况下，模型依赖于 **上一周的积分（Points_last_week）** 和 **排名变化趋势（Rank_change）** 等特征，能够非常准确地预测歌手的下周表现。模型在这样的情境下非常自信：“这首歌上周表现很好，趋势向上，所以我预测它下周会继续表现优异。”\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **温启动 (Warm Start / 新歌但歌手是大牌)**\n",
    "   - **MAE**: **23.82分** - 误差明显增大，但仍在可接受范围内。\n",
    "   - **R²**: **0.72** - 仍然不错，说明模型能够解释72%的分数变化。\n",
    "   - **Spearman**: **0.84** - 这是个亮点!!!！尽管积分预测不那么准确，但模型在预测新歌的相对排名方面依然非常出色。\n",
    "\n",
    "   **分析**：\n",
    "   对于新歌，尤其是那些由知名歌手演唱的歌曲，模型虽然没有完整的历史数据（例如 **Points_last_week** 特征为0），但它能够通过 **艺人热度（Artist_Hotness）** 特征来弥补这一不足。这一特征帮助模型判断：“我虽然不认识这首歌，但我认识它的歌手（如 Taylor Swift）。他的/她的其他歌曲表现出色，因此这首新歌的起点也不会低。” 这种 **“明星光环”效应** 为模型提供了有力的预测支持。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **真·冷启动 (True Cold Start / 新歌+新歌手)**\n",
    "   - **MAE**: **34.01分** - 误差非常大。\n",
    "   - **R²**: **-0.019** - 这是最关键的发现！R²为负数，意味着模型的预测结果甚至比直接猜所有歌曲的平均分还要差。\n",
    "   - **Spearman**: **0.36** - 非常低，说明模型几乎无法正确判断这些歌曲的相对排名。\n",
    "\n",
    "   **分析**：\n",
    "   在最极端的冷启动情况下，即 **新歌+新歌手**，模型的表现非常差。此时，模型既没有历史数据，也不知道歌手的知名度（**Artist_Hotness** 为0）。因此，模型只能依赖歌曲本身的音频特征（如 **Danceability**）进行预测，这显然是非常有限的。这个结果清楚地表明，音频特征本身几乎没有预测能力，真正决定一首歌能否成功的因素是它的历史表现和艺人的人气。\n",
    "\n",
    "---\n",
    "\n",
    "### 总结\n",
    "- **热启动（老歌）**：模型在已有历史数据的基础上表现优秀，能够高效预测歌曲的未来表现。\n",
    "- **温启动（新歌但歌手是大牌）**：虽然缺乏完整的历史数据，模型依然能够通过艺人热度等特征做出较为准确的预测。\n",
    "- **冷启动（新歌+新歌手）**：当模型缺乏关键特征时，其预测能力显著下降，特别是音频特征对预测的贡献非常有限。\n",
    "\n",
    "这些分析表明，**历史表现和艺人知名度** 是预测榜单成绩的关键因素，而**音频特征** 在冷启动时的作用相对较小。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
