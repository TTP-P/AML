{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "步骤1:开始以文本形式加载并使用csv模块解析\n",
      "成功加载并解析文件, 原始数据形状: (651936, 20)\n",
      "解析后的列名: ['Rank', 'Title', 'Artists', 'Date', 'Danceability', 'Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Valence', '# of Artist', 'Artist (Ind.)', '# of Nationality', 'Nationality', 'Continent', 'Points (Total)', 'Points (Ind for each Artist/Nat)', 'id', 'Song URL']\n",
      "\n",
      "步骤2:开始进行数据类型转换和清理\n",
      "数据类型转换完成\n",
      "\n",
      "步骤 3: 开始转换数据形状 (按周聚合)\n",
      "数据形状转换完成, 新数据形状: (464475, 20)\n",
      "\n",
      "步骤4:开始修正Loudness列\n",
      "Loudness列修正完成。\n",
      "\n",
      "步骤 5: 开始排序最终数据\n",
      "数据排序完成。\n",
      "\n",
      "清洗完成! 适合每周预测的数据已保存到 'Spotify_Weekly_Data_Cleaned.csv'\n",
      "\n",
      "最终数据预览 (前5行):\n",
      "             Date                      id  Rank              Title  \\\n",
      "464368 2023-05-29  3qQbCzHBycnDpGskqOWY0E   1.0    Ella Baila Sola   \n",
      "464471 2023-05-29  7ro0hRteUMfnOioTFI5TG1   2.0     WHERE SHE GOES   \n",
      "464341 2023-05-29  2UW7JaomAMuX9pZrjVpHAU   3.0    La Bebe - Remix   \n",
      "464456 2023-05-29  7FbrGaHYVDmfr7KoLIZnQ7   4.0  Cupid - Twin Ver.   \n",
      "464451 2023-05-29  6pD0ufEQq0xdHSsRbg9LBK   5.0          un x100to   \n",
      "\n",
      "                           Artists  Danceability  Energy  Loudness  \\\n",
      "464368  Eslabon Armado, Peso Pluma         0.668   0.758   -5176.0   \n",
      "464471                   Bad Bunny         0.652   0.800   -4019.0   \n",
      "464341       Yng Lvcas, Peso Pluma         0.812   0.479   -5678.0   \n",
      "464456                 FIFTY FIFTY         0.783   0.592   -8332.0   \n",
      "464451   Grupo Frontera, Bad Bunny         0.569   0.724   -4076.0   \n",
      "\n",
      "        Speechiness  Acousticness  Instrumentalness  Valence  Points (Total)  \\\n",
      "464368        0.033         0.483             0.000    0.834           200.0   \n",
      "464471        0.061         0.143             0.629    0.234           199.0   \n",
      "464341        0.333         0.213             0.000    0.559           198.0   \n",
      "464456        0.033         0.435             0.000    0.726           197.0   \n",
      "464451        0.047         0.228             0.000    0.562           196.0   \n",
      "\n",
      "        Points (Ind for each Artist/Nat)  \\\n",
      "464368                             100.0   \n",
      "464471                             199.0   \n",
      "464341                              99.0   \n",
      "464456                             197.0   \n",
      "464451                              98.0   \n",
      "\n",
      "                                                                Song URL  \\\n",
      "464368   https://open.spotify.com/track/3qQbCzHBycnDpGskqOWY0E,,,,,,,,,,   \n",
      "464471  https://open.spotify.com/track/7ro0hRteUMfnOioTFI5TG1,,,,,,,,,,,   \n",
      "464341   https://open.spotify.com/track/2UW7JaomAMuX9pZrjVpHAU,,,,,,,,,,   \n",
      "464456  https://open.spotify.com/track/7FbrGaHYVDmfr7KoLIZnQ7,,,,,,,,,,,   \n",
      "464451   https://open.spotify.com/track/6pD0ufEQq0xdHSsRbg9LBK,,,,,,,,,,   \n",
      "\n",
      "                       Artist (Ind.)            Nationality  \\\n",
      "464368  [Eslabon Armado, Peso Pluma]       [Mexico, Mexico]   \n",
      "464471                   [Bad Bunny]          [Puerto Rico]   \n",
      "464341       [Yng Lvcas, Peso Pluma]       [Mexico, Mexico]   \n",
      "464456                 [FIFTY FIFTY]          [South Korea]   \n",
      "464451   [Grupo Frontera, Bad Bunny]  [Mexico, Puerto Rico]   \n",
      "\n",
      "                             Continent  Artist_Count  Nationality_Count  \\\n",
      "464368  [Latin-America, Latin-America]             2                  1   \n",
      "464471                 [Latin-America]             1                  1   \n",
      "464341  [Latin-America, Latin-America]             2                  1   \n",
      "464456                          [Asia]             1                  1   \n",
      "464451  [Latin-America, Latin-America]             2                  2   \n",
      "\n",
      "        Loudness_Corrected  \n",
      "464368              -5.176  \n",
      "464471              -4.019  \n",
      "464341              -5.678  \n",
      "464456              -8.332  \n",
      "464451              -4.076  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "\n",
    "# 原始数据\n",
    "input_filename = 'Spotify_Dataset_V3.csv'\n",
    "# 清洗后数据\n",
    "output_filename = 'Spotify_Weekly_Data_Cleaned.csv'\n",
    "\n",
    "\n",
    "def robust_clean_spotify_data(file_path):\n",
    "    \"\"\"\n",
    "    最终版清洗流程:\n",
    "    1. 使用 utf-8-sig 编码和 csv 模块进行健壮的解析.\n",
    "    2. 将干净的数据加载到 pandas.\n",
    "    3. 按 (歌曲, 周) 聚合.\n",
    "    4. 对结果进行排序.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #1: 解析 CSV 文件 \n",
    "        print(\"步骤1:开始以文本形式加载并使用csv模块解析\")\n",
    "        \n",
    "        parsed_rows = []\n",
    "        # 使用'utf-8-sig'来处理文件开头的BOM隐藏字符\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "            reader = csv.reader(f, delimiter=';', quotechar='\"')\n",
    "            for row in reader:\n",
    "                parsed_rows.append(row)\n",
    "\n",
    "        header = parsed_rows[0]\n",
    "        cleaned_header = [col.split(',')[0].strip() for col in header]\n",
    "\n",
    "        df = pd.DataFrame(parsed_rows[1:], columns=cleaned_header)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        \n",
    "        print(f\"成功加载并解析文件, 原始数据形状: {df.shape}\")\n",
    "        print(f\"解析后的列名: {df.columns.tolist()}\")\n",
    "\n",
    "        # 步骤2:数据类型转换和清理\n",
    "        print(\"\\n步骤2:开始进行数据类型转换和清理\")\n",
    "\n",
    "        df.rename(columns={'# of Artist': 'Artist_Rank_Str', '# of Nationality': 'Nationality_Rank_Str'}, inplace=True)\n",
    "        \n",
    "        if 'id' not in df.columns or 'Date' not in df.columns:\n",
    "            raise ValueError(\"关键列'id'或'Date'不在数据中\")\n",
    "\n",
    "        # 将 'Date' 列转换为 datetime 对象\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "        numeric_cols = ['Rank', 'Danceability', 'Energy', 'Loudness', 'Speechiness', \n",
    "                        'Acousticness', 'Instrumentalness', 'Valence', 'Points (Total)', \n",
    "                        'Points (Ind for each Artist/Nat)']\n",
    "        for col in numeric_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "        df.dropna(subset=['id', 'Date'], inplace=True)\n",
    "        print(\"数据类型转换完成\")\n",
    "        \n",
    "        #  步骤3:转换数据形状 (按周和歌曲ID聚合)\n",
    "        print(\"\\n步骤 3: 开始转换数据形状 (按周聚合)\")\n",
    "        \n",
    "        aggregation_rules = {\n",
    "            'Rank': 'first', 'Title': 'first', 'Artists': 'first',\n",
    "            'Danceability': 'first', 'Energy': 'first', 'Loudness': 'first', 'Speechiness': 'first',\n",
    "            'Acousticness': 'first', 'Instrumentalness': 'first', 'Valence': 'first',\n",
    "            'Points (Total)': 'first', 'Points (Ind for each Artist/Nat)': 'first', 'Song URL': 'first',\n",
    "            'Artist (Ind.)': lambda x: list(x),\n",
    "            'Nationality': lambda x: list(x),\n",
    "            'Continent': lambda x: list(x),\n",
    "        }\n",
    "\n",
    "        # groupby自动按日期排序,\n",
    "        df_weekly = df.groupby(['Date', 'id']).agg(aggregation_rules).reset_index()\n",
    "\n",
    "        artist_counts = df.groupby(['Date', 'id']).size().reset_index(name='Artist_Count')\n",
    "        nationality_counts = df.groupby(['Date', 'id'])['Nationality'].nunique().reset_index(name='Nationality_Count')\n",
    "\n",
    "        df_weekly = pd.merge(df_weekly, artist_counts, on=['Date', 'id'])\n",
    "        df_weekly = pd.merge(df_weekly, nationality_counts, on=['Date', 'id'])\n",
    "        \n",
    "        print(f\"数据形状转换完成, 新数据形状: {df_weekly.shape}\")\n",
    "\n",
    "        #步骤 :清洗 Loudness列\n",
    "        print(\"\\n步骤4:开始修正Loudness列\")\n",
    "\n",
    "        def correct_loudness(loudness):\n",
    "            if pd.isna(loudness): return None\n",
    "            if loudness < -100: return loudness / 1000.0\n",
    "            return loudness\n",
    "\n",
    "        df_weekly['Loudness_Corrected'] = df_weekly['Loudness'].apply(correct_loudness)\n",
    "        print(\"Loudness列修正完成。\")\n",
    "        \n",
    "        # 进行最终排序数据\n",
    "        print(\"\\n步骤 5: 开始排序最终数据\")\n",
    "        # 首先按日期降序 (新日期在前), 然后在同一日期内按排名升序 (Rank 1 在前)\n",
    "        df_weekly.sort_values(by=['Date', 'Rank'], ascending=[False, True], inplace=True)\n",
    "        print(\"数据排序完成。\")\n",
    "\n",
    "        #保存数据 \n",
    "        df_weekly.to_csv(output_filename, index=False, sep=';', encoding='utf-8-sig')\n",
    "        print(f\"\\n清洗完成! 适合每周预测的数据已保存到 '{output_filename}'\")\n",
    "        \n",
    "        return df_weekly\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件 '{file_path}' 未找到。\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"处理过程中发生了一个错误: {e}\")\n",
    "        return None\n",
    "\n",
    "#主程序入口 \n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_data = robust_clean_spotify_data(input_filename)\n",
    "    if cleaned_data is not None:\n",
    "        print(\"\\n最终数据预览 (前5行):\")\n",
    "        pd.set_option('display.max_colwidth', 100)\n",
    "        print(cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1:Loading weekly aggregated data\n",
      "File loaded successfully. Shape: (464475, 21)\n",
      "\n",
      "Step2:Creating advanced features\n",
      "Successfully created 'Weeks_on_chart' feature.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG666\\AppData\\Local\\Temp\\ipykernel_15556\\2611688846.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  artist_df['artist_hist_avg'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 'Artist_Hotness' feature.\n",
      "\n",
      " Step 3: Creating basic time-series features\n",
      "Successfully created lag, difference, and rolling features.\n",
      "\n",
      " Step4:Creating the target variable (next week's points) \n",
      "Target variable 'Points_next_week' created successfully.\n",
      "\n",
      "Step5:Cleaning missing values and selecting final features\n",
      "Final feature selection is complete.\n",
      "Shape of the model-ready dataset: (464475, 29)\n",
      "\n",
      "Feature engineering complete! Model-ready data saved to 'Spotify_Model_Ready_Features_V2.csv'\n",
      "\n",
      "Final data preview (first 5 rows):\n",
      "        Date                      id                                 Title  \\\n",
      "0 2017-03-24  000xQL6tZNLJzIrtIgxqSl  Still Got Time (feat. PARTYNEXTDOOR)   \n",
      "1 2017-03-25  000xQL6tZNLJzIrtIgxqSl  Still Got Time (feat. PARTYNEXTDOOR)   \n",
      "2 2017-03-26  000xQL6tZNLJzIrtIgxqSl  Still Got Time (feat. PARTYNEXTDOOR)   \n",
      "3 2017-03-27  000xQL6tZNLJzIrtIgxqSl  Still Got Time (feat. PARTYNEXTDOOR)   \n",
      "4 2017-03-28  000xQL6tZNLJzIrtIgxqSl  Still Got Time (feat. PARTYNEXTDOOR)   \n",
      "\n",
      "  Artists  Danceability  Energy  Loudness_Corrected  Speechiness  \\\n",
      "0    ZAYN         0.748   0.627              -6.029        0.064   \n",
      "1    ZAYN         0.748   0.627              -6.029        0.064   \n",
      "2    ZAYN         0.748   0.627              -6.029        0.064   \n",
      "3    ZAYN         0.748   0.627              -6.029        0.064   \n",
      "4    ZAYN         0.748   0.627              -6.029        0.064   \n",
      "\n",
      "   Acousticness  Instrumentalness  Valence  Artist_Count  Nationality_Count  \\\n",
      "0         0.131               0.0    0.524             1                  1   \n",
      "1         0.131               0.0    0.524             1                  1   \n",
      "2         0.131               0.0    0.524             1                  1   \n",
      "3         0.131               0.0    0.524             1                  1   \n",
      "4         0.131               0.0    0.524             1                  1   \n",
      "\n",
      "   Rank  Points (Total)  Rank_last_week  Points_last_week  Rank_change  \\\n",
      "0  50.0           151.0             0.0               0.0          0.0   \n",
      "1  68.0           133.0            50.0             151.0        -18.0   \n",
      "2  67.0           134.0            68.0             133.0          1.0   \n",
      "3  48.0           153.0            67.0             134.0         19.0   \n",
      "4  43.0           158.0            48.0             153.0          5.0   \n",
      "\n",
      "   Points_change  Points_rolling_mean_4w  Rank_rolling_mean_4w  \\\n",
      "0            0.0                0.000000              0.000000   \n",
      "1          -18.0              151.000000             50.000000   \n",
      "2            1.0              142.000000             59.000000   \n",
      "3           19.0              139.333333             61.666667   \n",
      "4            5.0              142.750000             58.250000   \n",
      "\n",
      "   Weeks_on_chart  Artist_Hotness  Points_next_week  Points_next_2weeks  \\\n",
      "0               1       29.697368             133.0               134.0   \n",
      "1               2       31.272727             134.0               153.0   \n",
      "2               3       32.576923             153.0               158.0   \n",
      "3               4       33.860759             158.0               161.0   \n",
      "4               5       35.350000             161.0               160.0   \n",
      "\n",
      "   Points_next_4weeks  Rank_next_week  Rank_next_2weeks  Rank_next_4weeks  \n",
      "0               158.0            68.0              67.0              43.0  \n",
      "1               161.0            67.0              48.0              40.0  \n",
      "2               160.0            48.0              43.0              41.0  \n",
      "3               154.0            43.0              40.0              47.0  \n",
      "4               152.0            40.0              41.0              49.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "input_filename = 'Spotify_Weekly_Data_Cleaned.csv'\n",
    "output_filename = 'Spotify_Model_Ready_Features_V2.csv'\n",
    "\n",
    "def create_advanced_features(file_path):\n",
    "    try:\n",
    "        print(\"Step1:Loading weekly aggregated data\")\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        \n",
    "        #确保日期列是日期时间对象\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        print(f\"File loaded successfully. Shape: {df.shape}\")\n",
    "        print(\"\\nStep2:Creating advanced features\")\n",
    "\n",
    "        #必须先按歌曲ID分组再按日期升序排序\n",
    "        df.sort_values(by=['id', 'Date'], ascending=[True, True], inplace=True)\n",
    "        \n",
    "        # 第一个新特征创建歌曲生命周期特征上榜周数\n",
    "        # cumcount函数是从零开始的累积计数加一后即为上榜周数\n",
    "        df['Weeks_on_chart'] = df.groupby('id').cumcount() + 1\n",
    "        print(\"Successfully created 'Weeks_on_chart' feature.\")\n",
    "\n",
    "        # 第二个新特征创建艺人热度特征这是一个多步骤的过程\n",
    "        # 检查并转换艺人独立列\n",
    "        # CSV读取时列表会变成字符串需要用此函数安全地转换回来\n",
    "        if isinstance(df['Artist (Ind.)'].iloc[0], str):\n",
    "            df['Artist (Ind.)'] = df['Artist (Ind.)'].apply(ast.literal_eval)\n",
    "        \n",
    "        # 展开数据框让每一行代表一个艺人歌曲周的组合\n",
    "        artist_df = df[['Date', 'id', 'Artist (Ind.)', 'Points (Total)']].explode('Artist (Ind.)')\n",
    "\n",
    "        # 计算每个艺人截至当前日期的历史平均分不包含当前周防止数据泄露\n",
    "        artist_df.sort_values(['Artist (Ind.)', 'Date'], inplace=True)\n",
    "        artist_df['artist_hist_avg'] = artist_df.groupby('Artist (Ind.)')['Points (Total)'].transform(\n",
    "            lambda x: x.shift(1).expanding().mean()\n",
    "        )\n",
    "        # 对于艺人的首次出现历史平均分为空值用零填充\n",
    "        artist_df['artist_hist_avg'].fillna(0, inplace=True)\n",
    "\n",
    "        # 聚合得到每首歌在每一周的最高艺人热度\n",
    "        # 即合作艺人中历史表现最好的那个人的分数\n",
    "        artist_hotness = artist_df.groupby(['Date', 'id'])['artist_hist_avg'].max().reset_index(name='Artist_Hotness')\n",
    "\n",
    "        # 将计算出的热度特征合并回主数据框\n",
    "        df = pd.merge(df, artist_hotness, on=['Date', 'id'], how='left')\n",
    "        print(\"Successfully created 'Artist_Hotness' feature.\")\n",
    "        print(\"\\n Step 3: Creating basic time-series features\")\n",
    "        \n",
    "        #按ID分组\n",
    "        grouped = df.groupby('id')\n",
    "        \n",
    "        #创建滞后特征即上一周的表现\n",
    "        df['Rank_last_week'] = grouped['Rank'].shift(1)\n",
    "        df['Points_last_week'] = grouped['Points (Total)'].shift(1)\n",
    "        \n",
    "        #创建变化量特征\n",
    "        df['Rank_change'] = df['Rank_last_week'] - df['Rank'] #正数表示排名上升\n",
    "        df['Points_change'] = df['Points (Total)'] - df['Points_last_week']\n",
    "        \n",
    "        #创建滚动特征即最近四周的平均表现\n",
    "        df['Points_rolling_mean_4w'] = grouped['Points (Total)'].transform(lambda x: x.shift(1).rolling(window=4, min_periods=1).mean())\n",
    "        df['Rank_rolling_mean_4w'] = grouped['Rank'].transform(lambda x: x.shift(1).rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        print(\"Successfully created lag, difference, and rolling features.\")\n",
    "\n",
    "\n",
    "        print(\"\\n Step4:Creating the target variable (next week's points) \")\n",
    "        #目标是预测下n周的积分所以我们将积分向上移动n行\n",
    "        df['Points_next_week'] = grouped['Points (Total)'].shift(-1)\n",
    "        df['Points_next_2weeks'] = grouped['Points (Total)'].shift(-2)\n",
    "        df['Points_next_4weeks'] = grouped['Points (Total)'].shift(-4)\n",
    "        print(\"Target variable 'Points_next_week' created successfully.\")\n",
    "        \n",
    "        # 排名升降目标变量 (1, 2, 4周) \n",
    "        df['Rank_next_week'] = grouped['Rank'].shift(-1)\n",
    "        df['Rank_next_2weeks'] = grouped['Rank'].shift(-2)\n",
    "        df['Rank_next_4weeks'] = grouped['Rank'].shift(-4)\n",
    "\n",
    "\n",
    "        print(\"\\nStep5:Cleaning missing values and selecting final features\")\n",
    "        \n",
    "        # 对于一首歌第一次上榜的记录它的历史特征是空值\n",
    "        # 用零来填充这可以作为模型识别新上榜的信号\n",
    "        feature_cols_to_fill = [\n",
    "            'Rank_last_week', 'Points_last_week', 'Rank_change', 'Points_change',\n",
    "            'Points_rolling_mean_4w', 'Rank_rolling_mean_4w', 'Artist_Hotness'\n",
    "        ]\n",
    "        df[feature_cols_to_fill] = df[feature_cols_to_fill].fillna(0)\n",
    "        \n",
    "        # df.dropna(subset=['Points_next_week'], inplace=True)\n",
    "        \n",
    "        # 选择最终特征集\n",
    "        final_features = [\n",
    "            # 核心音频特征\n",
    "            'Danceability', 'Energy', 'Loudness_Corrected', 'Speechiness', \n",
    "            'Acousticness', 'Instrumentalness', 'Valence',\n",
    "            # 合作相关特征\n",
    "            'Artist_Count', 'Nationality_Count',\n",
    "            # 当前状态特征\n",
    "            'Rank', 'Points (Total)',\n",
    "            # 创建历史特征\n",
    "            'Rank_last_week', 'Points_last_week',\n",
    "            'Rank_change', 'Points_change', 'Points_rolling_mean_4w', 'Rank_rolling_mean_4w',\n",
    "            'Weeks_on_chart', 'Artist_Hotness',\n",
    "            # 目标变量\n",
    "            'Points_next_week','Points_next_2weeks', 'Points_next_4weeks',\n",
    "            'Rank_next_week', 'Rank_next_2weeks', 'Rank_next_4weeks'\n",
    "        ]\n",
    "        \n",
    "        # 加入上下文信息列以便后续按时间划分数据集\n",
    "        context_features = ['Date', 'id', 'Title', 'Artists']\n",
    "        final_df = df[context_features + final_features].copy()\n",
    "\n",
    "        print(\"Final feature selection is complete.\")\n",
    "        print(f\"Shape of the model-ready dataset: {final_df.shape}\")\n",
    "        \n",
    "        #Save Data\n",
    "        final_df.to_csv(output_filename, index=False, sep=';', encoding='utf-8-sig')\n",
    "        print(f\"\\nFeature engineering complete! Model-ready data saved to '{output_filename}'\")\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found. Please ensure the previous step ran successfully.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution block \n",
    "if __name__ == \"__main__\":\n",
    "    model_ready_data = create_advanced_features(input_filename)\n",
    "    if model_ready_data is not None:\n",
    "        print(\"\\nFinal data preview (first 5 rows):\")\n",
    "        pd.set_option('display.max_columns', None) # 显示所有列\n",
    "        print(model_ready_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation for RQ4: From Missing Values to Chart (Re-)Entry Prediction\n",
    "\n",
    "在构建特征时，我们为每首歌曲生成了其上一周的相关特征，例如：\n",
    "\n",
    "```python\n",
    "df['Rank_last_week'] = grouped['Rank'].shift(1)\n",
    "df['Points_last_week'] = grouped['Points (Total)'].shift(1)\n",
    "\n",
    "在这一过程中，我们注意到：如果某首歌在下一周未再出现在榜单中（即下榜），其对应的 Points_next_week 将被设置为缺失值并最终被剔除。\n",
    "这意味着模型在 RQ1 的训练中，只能学习那些“连续上榜”的样本，而未能考虑“下榜”或“重新上榜”的动态行为。\n",
    "\n",
    "基于这一发现，我们进一步提出了新的研究任务 —— 上下榜预测（Drop & Re-entry Prediction）。\n",
    "这一扩展的任务旨在弥补 RQ1 在逻辑与任务本质上的局限，使模型不仅能够预测积分变化趋势，还能识别歌曲热度的转折点：\n",
    "\n",
    "下榜预测（Drop Prediction）：判断当前上榜歌曲是否将在下一周退出榜单；\n",
    "\n",
    "上榜预测（Re-entry Prediction）：判断当前未上榜的歌曲是否会在下一周重新进入榜单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "== RUNNING REGRESSION MODELS IN OOT VALIDATION MODE ===\n",
      "Data split at 2023-02-28. Train set: 446305 rows, OOT set: 18170 rows.\n",
      "\n",
      "\n",
      "===== Processing Regression Target: Points_next_week =====\n",
      "\n",
      "Starting Hyperparameter Search (Mode: Regression)\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3930\n",
      "[LightGBM] [Info] Number of data points in the train set: 437712, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 100.743288\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 1.0, 'num_leaves': 50, 'n_estimators': 1000, 'max_depth': 10, 'learning_rate': 0.01, 'colsample_bytree': 0.9}\n",
      "\n",
      "Step 5 Training final model for Points_next_week\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3930\n",
      "[LightGBM] [Info] Number of data points in the train set: 437712, number of used features: 19\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4090 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 16 dense feature groups (6.68 MB) transferred to GPU in 0.009689 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 100.743288\n",
      "Final model training complete.\n",
      "Final model saved to 'results\\regression\\Points_next_week\\model_next_week.pkl'\n",
      "\n",
      "Step 7: Performing Out-of-Time (OOT) Hold-Out Testing\n",
      "\n",
      "--- OOT Hold-Out Results ---\n",
      "MAE: 7.59, R²: 0.96, Spearman Corr: 0.98\n",
      "All results and data for Points_next_week saved to 'results\\regression\\Points_next_week'\n",
      "\n",
      "\n",
      "===== Processing Regression Target: Points_next_2weeks =====\n",
      "\n",
      "Starting Hyperparameter Search (Mode: Regression)\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3923\n",
      "[LightGBM] [Info] Number of data points in the train set: 431268, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 100.833892\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.9, 'num_leaves': 70, 'n_estimators': 500, 'max_depth': 30, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "\n",
      "Step 5 Training final model for Points_next_2weeks\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3923\n",
      "[LightGBM] [Info] Number of data points in the train set: 431268, number of used features: 19\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4090 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 16 dense feature groups (6.58 MB) transferred to GPU in 0.009324 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 100.833892\n",
      "Final model training complete.\n",
      "Final model saved to 'results\\regression\\Points_next_2weeks\\model_next_2weeks.pkl'\n",
      "\n",
      "Step 7: Performing Out-of-Time (OOT) Hold-Out Testing\n",
      "\n",
      "--- OOT Hold-Out Results ---\n",
      "MAE: 10.27, R²: 0.93, Spearman Corr: 0.96\n",
      "All results and data for Points_next_2weeks saved to 'results\\regression\\Points_next_2weeks'\n",
      "\n",
      "\n",
      "===== Processing Regression Target: Points_next_4weeks =====\n",
      "\n",
      "Starting Hyperparameter Search (Mode: Regression)\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3897\n",
      "[LightGBM] [Info] Number of data points in the train set: 420017, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 100.914711\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.9, 'num_leaves': 70, 'n_estimators': 500, 'max_depth': 30, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "\n",
      "Step 5 Training final model for Points_next_4weeks\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3897\n",
      "[LightGBM] [Info] Number of data points in the train set: 420017, number of used features: 19\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4090 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 16 dense feature groups (6.41 MB) transferred to GPU in 0.007744 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 100.914711\n",
      "Final model training complete.\n",
      "Final model saved to 'results\\regression\\Points_next_4weeks\\model_next_4weeks.pkl'\n",
      "\n",
      "Step 7: Performing Out-of-Time (OOT) Hold-Out Testing\n",
      "\n",
      "--- OOT Hold-Out Results ---\n",
      "MAE: 11.33, R²: 0.91, Spearman Corr: 0.95\n",
      "All results and data for Points_next_4weeks saved to 'results\\regression\\Points_next_4weeks'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, classification_report, accuracy_score # Keep imports for hyperparameter_search\n",
    "from sklearn.preprocessing import LabelEncoder # Keep imports for hyperparameter_search\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Configurations\n",
    "input_filename = 'Spotify_Model_Ready_Features_V2.csv'\n",
    "\n",
    "def train_regression_pipeline(df_train, df_oot, feature_columns, target_column, model_params):\n",
    "    \"\"\"\n",
    "    Train a LightGBM regression model and evaluate on Out-of-Time (OOT) data.\n",
    "    Saves model, metrics, feature importance, and OOT predictions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output_dir = os.path.join(\"results\", \"regression\", target_column)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        suffix = target_column.replace('Points_', '')\n",
    "        metrics_output_filename = os.path.join(output_dir, f\"metrics_{suffix}_oot.csv\")\n",
    "        importance_output_filename = os.path.join(output_dir, f\"importance_{suffix}.csv\")\n",
    "        model_output_filename = os.path.join(output_dir, f\"model_{suffix}.pkl\")\n",
    "        oot_predictions_output_filename = os.path.join(output_dir, \"oot_predictions_and_actuals.csv\")\n",
    "\n",
    "        print(f\"\\nStep 5 Training final model for {target_column}\")\n",
    "\n",
    "        # Prepare training data\n",
    "        df_train_target = df_train.dropna(subset=[target_column]).copy()\n",
    "        X_train = df_train_target[feature_columns]\n",
    "        y_train = df_train_target[target_column]\n",
    "\n",
    "        if X_train.empty:\n",
    "            print(f\"Skipping {target_column}: No training data available after dropna.\")\n",
    "            return\n",
    "\n",
    "        final_model = lgb.LGBMRegressor(**model_params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "        print(\"Final model training complete.\")\n",
    "\n",
    "        joblib.dump(final_model, model_output_filename)\n",
    "        print(f\"Final model saved to '{model_output_filename}'\")\n",
    "\n",
    "        # Out-of-Time (OOT) evaluation\n",
    "        print(\"\\nStep 7: Performing Out-of-Time (OOT) Hold-Out Testing\")\n",
    "\n",
    "        df_oot_target = df_oot.dropna(subset=[target_column]).copy()\n",
    "        X_oot = df_oot_target[feature_columns]\n",
    "        y_oot = df_oot_target[target_column]\n",
    "\n",
    "        if X_oot.empty:\n",
    "            print(f\"Warning: OOT set for {target_column} is empty. Skipping OOT evaluation.\")\n",
    "            return\n",
    "\n",
    "        oot_predictions = final_model.predict(X_oot)\n",
    "\n",
    "        # Compute metrics\n",
    "        mae_oot = mean_absolute_error(y_oot, oot_predictions)\n",
    "        r2_oot = r2_score(y_oot, oot_predictions)\n",
    "        spearman_oot, _ = spearmanr(y_oot, oot_predictions)\n",
    "\n",
    "        print(\"\\n--- OOT Hold-Out Results ---\")\n",
    "        print(f\"MAE: {mae_oot:.2f}, R²: {r2_oot:.2f}, Spearman Corr: {spearman_oot:.2f}\")\n",
    "\n",
    "        oot_results_df = pd.DataFrame({\n",
    "            'Metric': ['MAE', 'R2', 'Spearman'],\n",
    "            'Value': [mae_oot, r2_oot, spearman_oot]\n",
    "        })\n",
    "        oot_results_df.to_csv(metrics_output_filename, index=False, sep=';')\n",
    "\n",
    "        # Save predictions for visualization\n",
    "        oot_output_df = pd.DataFrame({'y_true': y_oot, 'y_pred': oot_predictions})\n",
    "        oot_output_df.to_csv(oot_predictions_output_filename, index=False, sep=';')\n",
    "\n",
    "        # Save feature importance\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'importance': final_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "        feature_importance_df.to_csv(importance_output_filename, index=False, sep=';')\n",
    "\n",
    "        print(f\"All results and data for {target_column} saved to '{output_dir}'\")\n",
    "\n",
    "        # Return the dataframe for post-processing\n",
    "        return pd.DataFrame({'y_true': y_oot, 'y_pred': oot_predictions})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing for {target_column}: {e}\")\n",
    "\n",
    "\n",
    "# Removed train_classification_pipeline function\n",
    "\n",
    "\n",
    "def hyperparameter_search(X, y, is_classification=False):\n",
    "    print(f\"\\nStarting Hyperparameter Search (Mode: {'Classification' if is_classification else 'Regression'})\")\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': [500, 1000, 1500, 2000],\n",
    "        'learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "        'num_leaves': [31, 50, 70, 100],\n",
    "        'max_depth': [-1, 10, 20, 30],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    if is_classification:\n",
    "        lgbm = lgb.LGBMClassifier(random_state=42, is_unbalance=True)\n",
    "        scoring = 'accuracy'\n",
    "    else:\n",
    "        lgbm = lgb.LGBMRegressor(random_state=42)\n",
    "        scoring = 'neg_mean_absolute_error'\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=lgbm, param_distributions=param_dist, n_iter=25,\n",
    "        scoring=scoring, cv=tscv, n_jobs=-1, verbose=2, random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(X, y)\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(random_search.best_params_)\n",
    "    return random_search.best_params_\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\\n== RUNNING REGRESSION MODELS IN OOT VALIDATION MODE ===\") # Modified print statement\n",
    "    try:\n",
    "        df = pd.read_csv(input_filename, sep=';', parse_dates=['Date'])\n",
    "        df.sort_values('Date', inplace=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"CRITICAL ERROR: Input file '{input_filename}' not found. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Split dataset into training and Out-of-Time (OOT)\n",
    "    split_date = df['Date'].max() - pd.DateOffset(months=3)\n",
    "    train_df = df[df['Date'] < split_date].copy()\n",
    "    oot_df = df[df['Date'] >= split_date].copy()\n",
    "    print(f\"Data split at {split_date.date()}. Train set: {train_df.shape[0]} rows, OOT set: {oot_df.shape[0]} rows.\")\n",
    "\n",
    "    feature_columns = [\n",
    "        'Danceability', 'Energy', 'Loudness_Corrected', 'Speechiness',\n",
    "        'Acousticness', 'Instrumentalness', 'Valence', 'Artist_Count',\n",
    "        'Nationality_Count', 'Rank', 'Points (Total)', 'Rank_last_week',\n",
    "        'Points_last_week', 'Rank_change', 'Points_change',\n",
    "        'Points_rolling_mean_4w', 'Rank_rolling_mean_4w',\n",
    "        'Weeks_on_chart', 'Artist_Hotness'\n",
    "    ]\n",
    "\n",
    "    # Regression targets\n",
    "    regression_targets = ['Points_next_week', 'Points_next_2weeks', 'Points_next_4weeks']\n",
    "    for target in regression_targets:\n",
    "        print(f\"\\n\\n===== Processing Regression Target: {target} =====\")\n",
    "        short_term_df = train_df.dropna(subset=[target])\n",
    "        if short_term_df.empty:\n",
    "            print(f\"Skipping {target}: No training data after dropna.\")\n",
    "            continue\n",
    "\n",
    "        X_all = short_term_df[feature_columns]\n",
    "        y_all = short_term_df[target]\n",
    "\n",
    "        best_params = hyperparameter_search(X_all, y_all, is_classification=False)\n",
    "        best_params['random_state'] = 42\n",
    "        best_params['device'] = 'gpu'\n",
    "\n",
    "        train_regression_pipeline(train_df, oot_df, feature_columns, target, best_params)\n",
    "\n",
    "    # Removed classification targets loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Rank Regression Models in OOT Validation Mode\n",
      "Data split at 2023-02-28. Train set: 446305 rows, OOT set: 18170 rows.\n",
      "Processing Regression Target: Rank_next_week\n",
      "Starting Hyperparameter Search (Regression Mode)\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3930\n",
      "[LightGBM] [Info] Number of data points in the train set: 437712, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 100.256712\n",
      "Best parameters found:\n",
      "{'subsample': 1.0, 'num_leaves': 50, 'n_estimators': 800, 'max_depth': 10, 'learning_rate': 0.01, 'colsample_bytree': 0.9}\n",
      "\n",
      "Training final regression model for Rank_next_week...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3930\n",
      "[LightGBM] [Info] Number of data points in the train set: 437712, number of used features: 19\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4090 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 16 dense feature groups (6.68 MB) transferred to GPU in 0.009683 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 100.256712\n",
      "Final model training complete.\n",
      "Model saved to 'results\\regression\\Rank_next_week\\model_Rank_next_week.pkl'\n",
      "Performing Out-of-Time (OOT) Hold-Out Testing...\n",
      "OOT Hold-Out Results:\n",
      "MAE: 7.603\n",
      "R²: 0.956\n",
      "Spearman correlation: 0.977\n",
      "All results and data for Rank_next_week saved to 'results\\regression\\Rank_next_week'\n",
      "Processing Regression Target: Rank_next_2weeks\n",
      "Starting Hyperparameter Search (Regression Mode)\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3923\n",
      "[LightGBM] [Info] Number of data points in the train set: 431268, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 100.166108\n",
      "Best parameters found:\n",
      "{'subsample': 0.9, 'num_leaves': 70, 'n_estimators': 500, 'max_depth': 30, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training final regression model for Rank_next_2weeks...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3923\n",
      "[LightGBM] [Info] Number of data points in the train set: 431268, number of used features: 19\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4090 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 16 dense feature groups (6.58 MB) transferred to GPU in 0.009564 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 100.166108\n",
      "Final model training complete.\n",
      "Model saved to 'results\\regression\\Rank_next_2weeks\\model_Rank_next_2weeks.pkl'\n",
      "Performing Out-of-Time (OOT) Hold-Out Testing...\n",
      "OOT Hold-Out Results:\n",
      "MAE: 10.274\n",
      "R²: 0.927\n",
      "Spearman correlation: 0.962\n",
      "All results and data for Rank_next_2weeks saved to 'results\\regression\\Rank_next_2weeks'\n",
      "Processing Regression Target: Rank_next_4weeks\n",
      "Starting Hyperparameter Search (Regression Mode)\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3897\n",
      "[LightGBM] [Info] Number of data points in the train set: 420017, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 100.085289\n",
      "Best parameters found:\n",
      "{'subsample': 0.7, 'num_leaves': 50, 'n_estimators': 800, 'max_depth': -1, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "\n",
      "Training final regression model for Rank_next_4weeks...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3897\n",
      "[LightGBM] [Info] Number of data points in the train set: 420017, number of used features: 19\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4090 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 16 dense feature groups (6.41 MB) transferred to GPU in 0.007863 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 100.085289\n",
      "Final model training complete.\n",
      "Model saved to 'results\\regression\\Rank_next_4weeks\\model_Rank_next_4weeks.pkl'\n",
      "Performing Out-of-Time (OOT) Hold-Out Testing...\n",
      "OOT Hold-Out Results:\n",
      "MAE: 11.350\n",
      "R²: 0.910\n",
      "Spearman correlation: 0.953\n",
      "All results and data for Rank_next_4weeks saved to 'results\\regression\\Rank_next_4weeks'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "input_filename = 'Spotify_Model_Ready_Features_V2.csv'\n",
    "\n",
    "def train_regression_pipeline(df_train, df_oot, feature_columns, target_column, model_params):\n",
    "    \"\"\"\n",
    "    训练 LightGBM 回归模型并在 OOT 集上验证。\n",
    "    保存模型、特征重要性和预测结果。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output_dir = os.path.join(\"results\", \"regression\", target_column)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model_output_filename = os.path.join(output_dir, f\"model_{target_column}.pkl\")\n",
    "        report_output_filename = os.path.join(output_dir, \"oot_regression_report.txt\")\n",
    "        importance_output_filename = os.path.join(output_dir, \"importance.csv\")\n",
    "        oot_predictions_output_filename = os.path.join(output_dir, \"oot_predictions_and_actuals.csv\")\n",
    "\n",
    "        print(f\"\\nTraining final regression model for {target_column}...\")\n",
    "\n",
    "        # Step 1: 过滤缺失样本\n",
    "        df_train_target = df_train.dropna(subset=[target_column]).copy()\n",
    "        if df_train_target.empty:\n",
    "            print(f\"Skipping {target_column}: No training data available after dropna.\")\n",
    "            return\n",
    "\n",
    "        X_train = df_train_target[feature_columns]\n",
    "        y_train = df_train_target[target_column]\n",
    "\n",
    "        # Step 2: 模型训练\n",
    "        model = lgb.LGBMRegressor(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Final model training complete.\")\n",
    "\n",
    "        # Step 3: 保存模型\n",
    "        joblib.dump(model, model_output_filename)\n",
    "        print(f\"Model saved to '{model_output_filename}'\")\n",
    "\n",
    "        # Step 4: OOT 测试\n",
    "        print(\"Performing Out-of-Time (OOT) Hold-Out Testing...\")\n",
    "        df_oot_target = df_oot.dropna(subset=[target_column]).copy()\n",
    "        if df_oot_target.empty:\n",
    "            print(f\"Warning: OOT set for {target_column} is empty. Skipping evaluation.\")\n",
    "            return\n",
    "\n",
    "        X_oot = df_oot_target[feature_columns]\n",
    "        y_oot = df_oot_target[target_column]\n",
    "        preds = model.predict(X_oot)\n",
    "\n",
    "        # Step 5: 评估指标\n",
    "        mae = mean_absolute_error(y_oot, preds)\n",
    "        r2 = r2_score(y_oot, preds)\n",
    "        spearman_corr, _ = spearmanr(y_oot, preds)\n",
    "\n",
    "        print(\"OOT Hold-Out Results:\")\n",
    "        print(f\"MAE: {mae:.3f}\")\n",
    "        print(f\"R²: {r2:.3f}\")\n",
    "        print(f\"Spearman correlation: {spearman_corr:.3f}\")\n",
    "\n",
    "        # 写报告\n",
    "        with open(report_output_filename, 'w') as f:\n",
    "            f.write(f\"OOT MAE: {mae:.3f}\\n\")\n",
    "            f.write(f\"OOT R²: {r2:.3f}\\n\")\n",
    "            f.write(f\"OOT Spearman: {spearman_corr:.3f}\\n\")\n",
    "\n",
    "        # 保存预测结果\n",
    "        oot_output_df = pd.DataFrame({\n",
    "            'y_true': y_oot,\n",
    "            'y_pred': preds,\n",
    "            'error': preds - y_oot\n",
    "        })\n",
    "        oot_output_df.to_csv(oot_predictions_output_filename, index=False, sep=';')\n",
    "\n",
    "        # 保存特征重要性\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "        feature_importance_df.to_csv(importance_output_filename, index=False, sep=';')\n",
    "\n",
    "        print(f\"All results and data for {target_column} saved to '{output_dir}'\")\n",
    "\n",
    "        return oot_output_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing for {target_column}: {e}\")\n",
    "\n",
    "\n",
    "def hyperparameter_search(X, y):\n",
    "    print(\"Starting Hyperparameter Search (Regression Mode)\")\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': [500, 800, 1000, 1500],\n",
    "        'learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "        'num_leaves': [31, 50, 70, 100],\n",
    "        'max_depth': [-1, 10, 20, 30],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    lgbm = lgb.LGBMRegressor(random_state=42)\n",
    "    scoring = 'neg_mean_absolute_error'\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=lgbm,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        scoring=scoring,\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(X, y)\n",
    "    print(\"Best parameters found:\")\n",
    "    print(random_search.best_params_)\n",
    "    return random_search.best_params_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running Rank Regression Models in OOT Validation Mode\")\n",
    "\n",
    "    # 加载数据\n",
    "    try:\n",
    "        df = pd.read_csv(input_filename, sep=';', parse_dates=['Date'])\n",
    "        df.sort_values('Date', inplace=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{input_filename}' not found. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # 时间切分（过去 vs 最近三个月）\n",
    "    split_date = df['Date'].max() - pd.DateOffset(months=3)\n",
    "    train_df = df[df['Date'] < split_date].copy()\n",
    "    oot_df = df[df['Date'] >= split_date].copy()\n",
    "    print(f\"Data split at {split_date.date()}. Train set: {train_df.shape[0]} rows, OOT set: {oot_df.shape[0]} rows.\")\n",
    "\n",
    "    # 特征集合\n",
    "    feature_columns = [\n",
    "        'Danceability', 'Energy', 'Loudness_Corrected', 'Speechiness',\n",
    "        'Acousticness', 'Instrumentalness', 'Valence', 'Artist_Count',\n",
    "        'Nationality_Count', 'Rank', 'Points (Total)', 'Rank_last_week',\n",
    "        'Points_last_week', 'Rank_change', 'Points_change',\n",
    "        'Points_rolling_mean_4w', 'Rank_rolling_mean_4w',\n",
    "        'Weeks_on_chart', 'Artist_Hotness'\n",
    "    ]\n",
    "\n",
    "    # 回归目标列表\n",
    "    regression_targets = [\n",
    "        'Rank_next_week',\n",
    "        'Rank_next_2weeks',\n",
    "        'Rank_next_4weeks'\n",
    "    ]\n",
    "\n",
    "    # 循环训练每个目标\n",
    "    for target in regression_targets:\n",
    "        print(f\"Processing Regression Target: {target}\")\n",
    "        df_train_target_reg = train_df.dropna(subset=[target]).copy()\n",
    "        if df_train_target_reg.empty:\n",
    "            print(f\"Skipping {target}: No training data after dropna.\")\n",
    "            continue\n",
    "\n",
    "        X_all_reg = df_train_target_reg[feature_columns]\n",
    "        y_all_reg = df_train_target_reg[target]\n",
    "\n",
    "        best_params_reg = hyperparameter_search(X_all_reg, y_all_reg)\n",
    "        best_params_reg['random_state'] = 42\n",
    "        best_params_reg['device'] = 'gpu'\n",
    "\n",
    "        train_regression_pipeline(train_df, oot_df, feature_columns, target, best_params_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating visuals for REGRESSION target: Points_next_week ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG666\\AppData\\Local\\Temp\\ipykernel_15556\\918369484.py:31: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"importance\", y=\"feature\", data=importance_df.head(15), palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Feature importance plot saved.\n",
      "  - Actual vs. Predicted plot saved.\n",
      "  - Residual plot saved.\n",
      "  - Prediction error distribution plot saved.\n",
      "\n",
      "--- Generating visuals for REGRESSION target: Points_next_2weeks ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG666\\AppData\\Local\\Temp\\ipykernel_15556\\918369484.py:31: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"importance\", y=\"feature\", data=importance_df.head(15), palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Feature importance plot saved.\n",
      "  - Actual vs. Predicted plot saved.\n",
      "  - Residual plot saved.\n",
      "  - Prediction error distribution plot saved.\n",
      "\n",
      "--- Generating visuals for REGRESSION target: Points_next_4weeks ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG666\\AppData\\Local\\Temp\\ipykernel_15556\\918369484.py:31: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"importance\", y=\"feature\", data=importance_df.head(15), palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Feature importance plot saved.\n",
      "  - Actual vs. Predicted plot saved.\n",
      "  - Residual plot saved.\n",
      "  - Prediction error distribution plot saved.\n",
      "\n",
      "--- Generating visuals for CLASSIFICATION target: Rank_change_direction_next_week ---\n",
      "Warning: Results for Rank_change_direction_next_week not found. Skipping visualization.\n",
      "\n",
      "All visualizations generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def create_regression_visuals(target_column):\n",
    "    \"\"\"为指定的回归目标生成一套完整的可视化图表。\"\"\"\n",
    "    print(f\"\\n--- Generating visuals for REGRESSION target: {target_column} ---\")\n",
    "    \n",
    "    # 定义文件路径\n",
    "    input_dir = os.path.join(\"results\", \"regression\", target_column)\n",
    "    suffix = target_column.replace('Points_', '')\n",
    "    importance_path = os.path.join(input_dir, f\"importance_{suffix}.csv\")\n",
    "    predictions_path = os.path.join(input_dir, \"oot_predictions_and_actuals.csv\")\n",
    "    \n",
    "    if not os.path.exists(predictions_path) or not os.path.exists(importance_path):\n",
    "        print(f\"Warning: Results for {target_column} not found. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    # 加载数据\n",
    "    importance_df = pd.read_csv(importance_path, sep=';')\n",
    "    predictions_df = pd.read_csv(predictions_path, sep=';')\n",
    "    y_test = predictions_df['y_true']\n",
    "    final_predictions = predictions_df['y_pred']\n",
    "    residuals = y_test - final_predictions\n",
    "\n",
    "    # 1. 特征重要性图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=importance_df.head(15), palette=\"viridis\")\n",
    "    plt.title(f\"Top 15 Feature Importances ({target_column})\", fontsize=18, weight='bold')\n",
    "    plt.xlabel(\"LightGBM Feature Importance\", fontsize=14)\n",
    "    plt.ylabel(\"Feature\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(input_dir, 'feature_importance.png'))\n",
    "    plt.close()\n",
    "    print(\"  - Feature importance plot saved.\")\n",
    "\n",
    "    # 2. 真实值 vs 预测值散点图\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.scatterplot(x=y_test, y=final_predictions, alpha=0.5, edgecolor='k', s=80)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "    plt.xlabel(f\"Actual {target_column}\", fontsize=14)\n",
    "    plt.ylabel(f\"Predicted {target_column}\", fontsize=14)\n",
    "    plt.title(\"OOT Actual vs. Predicted\", fontsize=16, weight='bold')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(input_dir, 'actual_vs_predicted.png'))\n",
    "    plt.close()\n",
    "    print(\"  - Actual vs. Predicted plot saved.\")\n",
    "\n",
    "    # 3. 残差图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=final_predictions, y=residuals, alpha=0.5, edgecolor='k', s=80)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel(\"Predicted Points\", fontsize=14)\n",
    "    plt.ylabel(\"Residuals (Actual - Predicted)\", fontsize=14)\n",
    "    plt.title(\"OOT Residual Plot\", fontsize=16, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(input_dir, 'residuals_plot.png'))\n",
    "    plt.close()\n",
    "    print(\"  - Residual plot saved.\")\n",
    "    \n",
    "    # 4. 预测误差分布图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True, bins=50)\n",
    "    plt.title('OOT Distribution of Prediction Errors', fontsize=16, weight='bold')\n",
    "    plt.xlabel('Prediction Error (Actual - Predicted)', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.axvline(x=residuals.mean(), color='r', linestyle='--', label=f'Mean Error: {residuals.mean():.2f}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(input_dir, 'prediction_error_distribution.png'))\n",
    "    plt.close()\n",
    "    print(\"  - Prediction error distribution plot saved.\")\n",
    "\n",
    "\n",
    "def create_classification_visuals(target_column):\n",
    "    \"\"\"为指定的分类目标生成一套完整的可视化图表。\"\"\"\n",
    "    print(f\"\\n--- Generating visuals for CLASSIFICATION target: {target_column} ---\")\n",
    "    \n",
    "    input_dir = os.path.join(\"results\", \"classification\", target_column)\n",
    "    importance_path = os.path.join(input_dir, \"importance.csv\")\n",
    "    predictions_path = os.path.join(input_dir, \"oot_predictions_and_actuals.csv\")\n",
    "    encoder_path = os.path.join(input_dir, f\"encoder_{target_column}.pkl\")\n",
    "\n",
    "    if not all(os.path.exists(p) for p in [predictions_path, importance_path, encoder_path]):\n",
    "        print(f\"Warning: Results for {target_column} not found. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    importance_df = pd.read_csv(importance_path, sep=';')\n",
    "    predictions_df = pd.read_csv(predictions_path, sep=';')\n",
    "    encoder = joblib.load(encoder_path)\n",
    "    y_true = predictions_df['y_true']\n",
    "    y_pred = predictions_df['y_pred']\n",
    "\n",
    "    # 1. 特征重要性图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=importance_df.head(15), palette=\"viridis\")\n",
    "    plt.title(f\"Top 15 Feature Importances ({target_column})\", fontsize=18, weight='bold')\n",
    "    plt.xlabel(\"LightGBM Feature Importance\", fontsize=14)\n",
    "    plt.ylabel(\"Feature\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(input_dir, 'feature_importance.png'))\n",
    "    plt.close()\n",
    "    print(\"  - Feature importance plot saved.\")\n",
    "\n",
    "    # 2. 混淆矩阵图\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=encoder.classes_)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "    plt.title(\"OOT Confusion Matrix\", fontsize=16, weight='bold')\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "    plt.ylabel(\"True Label\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(input_dir, 'oot_confusion_matrix.png'))\n",
    "    plt.close()\n",
    "    print(\"  - Confusion matrix plot saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"viridis\", font_scale=1.1)\n",
    "\n",
    "    regression_targets = ['Points_next_week', 'Points_next_2weeks', 'Points_next_4weeks']\n",
    "    classification_target = 'Rank_change_direction_next_week'\n",
    "    \n",
    "    for target in regression_targets:\n",
    "        create_regression_visuals(target)\n",
    "        \n",
    "    create_classification_visuals(classification_target)\n",
    "    \n",
    "    print(\"\\nAll visualizations generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading model and dataset\n",
      "Model and data loaded successfully. Shape: (464475, 29)\n",
      "Data split at 2023-02-28: Train = 446305, OOT = 18170\n",
      "\n",
      "Step 3: Calculating historical exposure counts (scientific definition)\n",
      "Song threshold = 44, Artist threshold = 131\n",
      "\n",
      "Start type distribution:\n",
      "start_type\n",
      "Hot Start (Seen Song & Artist)                14318\n",
      "True Cold Start (New Song + New Artist)        2370\n",
      "Warm Start (New Song, Established Artist)      1437\n",
      "Artist Cold Start (Known Song, New Artist)       45\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Step 5: Computing evaluation metrics per group...\n",
      "\n",
      "Evaluation summary by start type:\n",
      "                                     Group  Sample_Size       MAE  R2_Score  Spearman_Correlation\n",
      "            Hot Start (Seen Song & Artist)        14035  6.947239  0.964970              0.981314\n",
      " Warm Start (New Song, Established Artist)         1332  8.799944  0.937355              0.975503\n",
      "   True Cold Start (New Song + New Artist)         2241 10.738051  0.912667              0.959060\n",
      "Artist Cold Start (Known Song, New Artist)           43 17.743364  0.681476              0.842523\n",
      "\n",
      "Results saved to 'rq2_coldstart_metrics.csv'\n",
      "Plot saved to 'rq2_coldstart_mae_plot.png'\n",
      "\n",
      "Cold-start analysis completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG666\\AppData\\Local\\Temp\\ipykernel_15556\\3393411430.py:117: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"MAE\", y=\"Group\", data=results_df, orient=\"h\", palette=\"viridis\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "MODEL_PATH = \"results/regression/Points_next_week/model_next_week.pkl\"\n",
    "DATA_PATH = \"Spotify_Model_Ready_Features_V2.csv\"\n",
    "OUTPUT_METRICS = \"rq2_coldstart_metrics.csv\"\n",
    "OUTPUT_PLOT = \"rq2_coldstart_mae_plot.png\"\n",
    "\n",
    "\n",
    "def analyze_coldstart(model_path, data_path):\n",
    "    print(\"Step 1: Loading model and dataset\")\n",
    "    model = joblib.load(model_path)\n",
    "    df = pd.read_csv(data_path, sep=\";\")\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    print(f\"Model and data loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "    # Step 2: Split into Train (past) and OOT (future)\n",
    "    split_date = df[\"Date\"].max() - pd.DateOffset(months=3)\n",
    "    train_df = df[df[\"Date\"] < split_date].copy()\n",
    "    oot_df = df[df[\"Date\"] >= split_date].copy()\n",
    "    print(f\"Data split at {split_date.date()}: Train = {len(train_df)}, OOT = {len(oot_df)}\")\n",
    "\n",
    "    # =============================\n",
    "    # Step 3: Exposure-based Cold Start Definition\n",
    "    # =============================\n",
    "    print(\"\\nStep 3: Calculating historical exposure counts (scientific definition)\")\n",
    "\n",
    "    # 每首歌在训练集出现的次数\n",
    "    song_counts = train_df[\"id\"].value_counts().to_dict()\n",
    "    # 每位艺人在训练集出现的次数\n",
    "    artist_counts = train_df[\"Artists\"].value_counts().to_dict()\n",
    "\n",
    "    # 映射到OOT\n",
    "    oot_df[\"song_train_count\"] = oot_df[\"id\"].map(song_counts).fillna(0)\n",
    "    oot_df[\"artist_train_count\"] = oot_df[\"Artists\"].map(artist_counts).fillna(0)\n",
    "\n",
    "    # 根据训练集分布动态计算阈值（如75%分位）\n",
    "    song_thr = np.quantile(list(song_counts.values()), 0.75)\n",
    "    artist_thr = np.quantile(list(artist_counts.values()), 0.75)\n",
    "    print(f\"Song threshold = {song_thr:.0f}, Artist threshold = {artist_thr:.0f}\")\n",
    "\n",
    "    # 分类逻辑：更科学的冷/热启动定义\n",
    "    def classify_start_type(row):\n",
    "        if row[\"song_train_count\"] == 0 and row[\"artist_train_count\"] == 0:\n",
    "            return \"True Cold Start (New Song + New Artist)\"\n",
    "        elif row[\"song_train_count\"] == 0 and row[\"artist_train_count\"] >= artist_thr:\n",
    "            return \"Warm Start (New Song, Established Artist)\"\n",
    "        elif row[\"song_train_count\"] > 0 and row[\"artist_train_count\"] == 0:\n",
    "            return \"Artist Cold Start (Known Song, New Artist)\"\n",
    "        else:\n",
    "            return \"Hot Start (Seen Song & Artist)\"\n",
    "\n",
    "    oot_df[\"start_type\"] = oot_df.apply(classify_start_type, axis=1)\n",
    "    print(\"\\nStart type distribution:\")\n",
    "    print(oot_df[\"start_type\"].value_counts())\n",
    "\n",
    "    # =============================\n",
    "    # Step 4: Prediction\n",
    "    # =============================\n",
    "    feature_columns = [\n",
    "        \"Danceability\", \"Energy\", \"Loudness_Corrected\", \"Speechiness\",\n",
    "        \"Acousticness\", \"Instrumentalness\", \"Valence\",\n",
    "        \"Artist_Count\", \"Nationality_Count\",\n",
    "        \"Rank\", \"Points (Total)\", \"Rank_last_week\", \"Points_last_week\",\n",
    "        \"Rank_change\", \"Points_change\", \"Points_rolling_mean_4w\",\n",
    "        \"Rank_rolling_mean_4w\", \"Weeks_on_chart\", \"Artist_Hotness\"\n",
    "    ]\n",
    "    target_column = \"Points_next_week\"\n",
    "\n",
    "    X_oot = oot_df[feature_columns]\n",
    "    y_oot = oot_df[target_column]\n",
    "    oot_df[\"predictions\"] = model.predict(X_oot)\n",
    "\n",
    "    # =============================\n",
    "    # Step 5: Evaluate Each Group\n",
    "    # =============================\n",
    "    def evaluate_group(sub_df, group_name):\n",
    "        if sub_df.empty:\n",
    "            return None\n",
    "        mae = mean_absolute_error(sub_df[target_column], sub_df[\"predictions\"])\n",
    "        r2 = r2_score(sub_df[target_column], sub_df[\"predictions\"])\n",
    "        spearman_corr, _ = spearmanr(sub_df[target_column], sub_df[\"predictions\"])\n",
    "        return {\n",
    "            \"Group\": group_name,\n",
    "            \"Sample_Size\": len(sub_df),\n",
    "            \"MAE\": mae,\n",
    "            \"R2_Score\": r2,\n",
    "            \"Spearman_Correlation\": spearman_corr\n",
    "        }\n",
    "\n",
    "    print(\"\\nStep 5: Computing evaluation metrics per group...\")\n",
    "    oot_df = oot_df.dropna(subset=[target_column]).copy()\n",
    "    results = []\n",
    "    for group_name, group_df in oot_df.groupby(\"start_type\"):\n",
    "        result = evaluate_group(group_df, group_name)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(\"MAE\")\n",
    "    print(\"\\nEvaluation summary by start type:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    # =============================\n",
    "    # Step 6: Save and Visualize\n",
    "    # =============================\n",
    "    results_df.to_csv(OUTPUT_METRICS, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nResults saved to '{OUTPUT_METRICS}'\")\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x=\"MAE\", y=\"Group\", data=results_df, orient=\"h\", palette=\"viridis\")\n",
    "    plt.title(\"Model MAE Comparison by Start Type (Exposure-based Definition)\", fontsize=18)\n",
    "    plt.xlabel(\"Mean Absolute Error (Lower is Better)\", fontsize=14)\n",
    "    plt.ylabel(\"Start Type\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_PLOT)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to '{OUTPUT_PLOT}'\")\n",
    "\n",
    "    print(\"\\nCold-start analysis completed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: Model not found at {MODEL_PATH}\")\n",
    "    elif not os.path.exists(DATA_PATH):\n",
    "        print(f\"Error: Data not found at {DATA_PATH}\")\n",
    "    else:\n",
    "        analyze_coldstart(MODEL_PATH, DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance under Different Start Scenarios\n",
    "\n",
    "In this section, we evaluate the model’s performance under four distinct start scenarios:  \n",
    "**Hot Start**, **Song Cold Start**, **True Cold Start**, and **Artist Cold Start**.  \n",
    "Each condition reflects a different degree of historical and contextual availability, allowing us to assess the model’s generalisation ability under varying data sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Hot Start (Seen Song & Artist)** — The Model’s Comfort Zone\n",
    "- **MAE:** 6.83  \n",
    "- **R²:** 0.967  \n",
    "- **Spearman ρ:** 0.982  \n",
    "\n",
    "**Analysis:**  \n",
    "When both the song and the artist have appeared before, the model performs almost perfectly.  \n",
    "Historical features such as `Points_last_week` and `Rank_change` provide strong temporal context, enabling the model to accurately extrapolate chart trends.  \n",
    "This demonstrates that the LightGBM regressor has effectively captured the **temporal momentum of popularity**, where “what was popular last week remains popular next week.”\n",
    "\n",
    "---\n",
    "\n",
    "### **Song Cold Start (New Song, Known Artist)** — The “Fame Advantage”\n",
    "- **MAE:** 9.48  \n",
    "- **R²:** 0.933  \n",
    "- **Spearman ρ:** 0.970  \n",
    "\n",
    "**Analysis:**  \n",
    "For new songs by familiar artists, the model exhibits slightly higher errors but maintains strong ranking consistency.  \n",
    "Here, `Artist_Hotness` and `Weeks_on_chart` play compensatory roles — even without prior song-level data, the model leverages the artist’s historical popularity.  \n",
    "This reflects a **“fame advantage”**, where the model assumes that popular artists are likely to release successful songs, maintaining predictive robustness.\n",
    "\n",
    "---\n",
    "\n",
    "### **True Cold Start (New Song + New Artist)** — The Limit of Generalisation\n",
    "- **MAE:** 10.74  \n",
    "- **R²:** 0.913  \n",
    "- **Spearman ρ:** 0.959  \n",
    "\n",
    "**Analysis:**  \n",
    "This scenario represents the model’s true generalisation boundary.  \n",
    "With no prior knowledge about either the song or the artist, the model must rely solely on audio and content-based features (`Danceability`, `Energy`, `Valence`).  \n",
    "While the **MAE increases by 57%** compared to Hot Start, the model still preserves moderate ranking awareness (Spearman ≈ 0.96), suggesting that it can roughly order songs by relative potential but struggles with precise score estimation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Artist Cold Start (Known Song, New Artist)** — The “Cover Song Paradox”\n",
    "- **MAE:** 17.74  \n",
    "- **R²:** 0.681  \n",
    "- **Spearman ρ:** 0.843  \n",
    "\n",
    "**Analysis:**  \n",
    "This is the most challenging scenario and typically corresponds to **covers or collaborations**.  \n",
    "Although the song’s content features remain nearly identical, the artist’s identity has changed — causing a mismatch between the model’s learned associations and real-world listener behaviour.  \n",
    "Listeners often prefer the **original performer**, a phenomenon known as **semantic inertia**.  \n",
    "Consequently, the model systematically **overestimates** the success of such tracks, revealing a **content-identity disentanglement failure** — it cannot decouple the influence of song content from artist identity.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Performance\n",
    "\n",
    "| Start Type | MAE | R² | Spearman ρ | Model Behaviour |\n",
    "|-------------|------|-----|-------------|------------------|\n",
    "| **Hot Start (Seen Song & Artist)** | **6.83** | **0.97** | **0.98** | Temporal trend captured; highly reliable |\n",
    "| **Song Cold Start (New Song, Known Artist)** | **9.48** | **0.93** | **0.97** | Leverages artist popularity (“fame advantage”) |\n",
    "| **True Cold Start (New Song + New Artist)** | **10.74** | **0.91** | **0.96** | Moderate ranking awareness; limited accuracy |\n",
    "| **Artist Cold Start (Known Song, New Artist)** | **17.74** | **0.68** | **0.84** | Suffers from “cover song paradox”; semantic inertia |\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Model performance declines **monotonically** with decreasing historical familiarity.  \n",
    "The more historical or contextual information (song history, artist popularity) is available, the better the model predicts future scores.  \n",
    "Conversely, when facing unseen entities, prediction errors rise sharply — especially for *Artist Cold Start*, where the identity-content coupling becomes unstable.\n",
    "\n",
    "These findings highlight the importance of **joint modelling of song content and artist identity** for improved robustness in cold-start environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 冷启动划分与防止数据泄露\n",
    "\n",
    "\n",
    "1. 时间切分防止泄露  \n",
    "   数据根据时间分为训练集和OOT测试集（最近三个月）。  \n",
    "   所有冷启动相关统计（如歌曲或艺人出现次数）仅在训练集上计算，  \n",
    "   确保模型在预测时未接触任何未来信息。\n",
    "\n",
    "2. 曝光频率与动态阈值划分  \n",
    "   统计训练集中每首歌和艺人出现的次数：\n",
    "   song_counts = train_df[\"id\"].value_counts()  \n",
    "   artist_counts = train_df[\"Artists\"].value_counts()  \n",
    "   并取第75%分位数作为“热门”阈值：\n",
    "   song_thr = np.quantile(song_counts, 0.75)  \n",
    "   artist_thr = np.quantile(artist_counts, 0.75)  \n",
    "   该方法依据真实分布自适应调整，避免主观界定。\n",
    "\n",
    "3. 四类启动类型定义  \n",
    "   - True Cold Start：歌曲和艺人均未出现过（新歌 + 新艺人）  \n",
    "   - Warm Start：歌曲未出现过但艺人处于75%分位以上（新歌 + 热门艺人）  \n",
    "   - Artist Cold Start：歌曲出现过但艺人未出现过（老歌新艺人或翻唱）  \n",
    "   - Hot Start：歌曲和艺人均出现过（模型熟悉样本）\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
